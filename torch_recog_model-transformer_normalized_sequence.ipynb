{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, x_set, y_set, seq_len):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.seq_len)))\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = idx * self.seq_len\n",
    "        end_idx = (idx + 1) * self.seq_len\n",
    "\n",
    "        batch_x = self.x[start_idx:end_idx]\n",
    "        batch_y = self.y[start_idx:end_idx]\n",
    "\n",
    "        # Count the occurrences of each row\n",
    "        unique_rows, counts = np.unique(batch_y, axis=0, return_counts=True)\n",
    "\n",
    "        # Get the index of the row with the highest count\n",
    "        most_common_row_index = np.argmax(counts)\n",
    "\n",
    "        # Get the most common row\n",
    "        most_common_row = unique_rows[most_common_row_index]\n",
    "        \n",
    "        \n",
    "        batch_y_bin = most_common_row\n",
    "\n",
    "        # Convert NumPy arrays to PyTorch tensors\n",
    "        batch_x = torch.from_numpy(batch_x)\n",
    "        batch_y = torch.from_numpy(batch_y)\n",
    "        batch_y_bin = torch.from_numpy(batch_y_bin)\n",
    "\n",
    "        # Pad sequences to ensure they have the same length within the batch\n",
    "        pad_len = self.seq_len - batch_x.shape[0]\n",
    "        if pad_len > 0:\n",
    "            pad_shape = (pad_len,) + batch_x.shape[1:]\n",
    "            pad_shape_y = (pad_len,) + batch_y.shape[1:]\n",
    "\n",
    "            batch_x = torch.cat([batch_x, torch.zeros(pad_shape)], dim=0)\n",
    "            batch_y = torch.cat([batch_y, torch.zeros(pad_shape_y)], dim=0)\n",
    "\n",
    "        return batch_x, batch_y, batch_y_bin\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        indices = np.arange(len(self.x))\n",
    "        np.random.shuffle(indices)\n",
    "        self.x = self.x[indices]\n",
    "        self.y = self.y[indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8])\n"
     ]
    }
   ],
   "source": [
    "# target output size of 5\n",
    "\n",
    "class GlobalMaxPooling1D(nn.Module):\n",
    "\n",
    "    def __init__(self, data_format='channels_last'):\n",
    "        super(GlobalMaxPooling1D, self).__init__()\n",
    "        self.data_format = data_format\n",
    "        self.step_axis = 1 if self.data_format == 'channels_last' else 2\n",
    "\n",
    "    def forward(self, input):\n",
    "        return torch.max(input, axis=self.step_axis).values\n",
    "    \n",
    "\n",
    "m = GlobalMaxPooling1D()\n",
    "input = torch.randn(1, 64, 8)\n",
    "output = m(input)\n",
    "print(output.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, d_model, nhead, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout), num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        self.max_pool = GlobalMaxPooling1D()\n",
    "        self.fc = nn.Linear(input_dim, d_model)\n",
    "        self.out = nn.Linear(d_model, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        x = self.max_pool(x)\n",
    "  \n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Subject Trials:  32\n",
      "Test Subject Trials:  4\n",
      "[ 7.56010000e-02  2.07810000e-02 -6.73910000e-02 -9.10000000e-04\n",
      "  1.50000000e-04 -4.00000000e-04 -3.72609969e-02 -6.45423363e-01\n",
      "  1.94898219e-01  7.37600831e-01]\n",
      "[ 7.52001269e-02  2.06708091e-02 -6.70336603e-02 -9.05174739e-04\n",
      "  1.49204627e-04 -3.97879006e-04 -3.70634210e-02 -6.42001015e-01\n",
      "  1.93864774e-01  7.33689713e-01]\n",
      "0 torch.Size([30, 30, 10]) torch.Size([30, 30, 14]) torch.Size([30, 14])\n"
     ]
    }
   ],
   "source": [
    "def generate_data(subject_id, task, features, batch_size, seq_len):    \n",
    "    \n",
    "    csv_path = './ProcessedDatasets/' + task\n",
    "    # csv_path = './Dataset'\n",
    "    \n",
    "    csv_files = glob.glob(csv_path + \"/*.csv\")\n",
    "    \n",
    "    train_df_list = []\n",
    "    test_df_list = []\n",
    "    \n",
    "    for file in csv_files:\n",
    "        if(subject_id in file):\n",
    "            test_df_list.append(pd.read_csv(file))\n",
    "#             print(file)\n",
    "        else:\n",
    "            train_df_list.append(pd.read_csv(file))\n",
    "            \n",
    "\n",
    "    print('Train Subject Trials: ',len(train_df_list))\n",
    "    print('Test Subject Trials: ',len(test_df_list))\n",
    "    \n",
    "    # Concatenate all DataFrames\n",
    "    train_df   = pd.concat(train_df_list, ignore_index=True)\n",
    "    test_df   = pd.concat(test_df_list, ignore_index=True)\n",
    "\n",
    "    \n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "\n",
    "    train_labels= train_df.pop('label')\n",
    "    train_features = train_df\n",
    "\n",
    "    test_labels= test_df.pop('label')\n",
    "    test_features = test_df\n",
    "\n",
    "\n",
    "    all_class_names = [\"G1\", 'G2', 'G3', 'G4', 'G5', 'G6', 'G8', 'G9', 'G10', 'G11', 'G12', 'G13', 'G14', 'G15']\n",
    "    lb.fit(all_class_names)\n",
    "\n",
    "    train_labels = lb.transform(train_labels)\n",
    "    test_labels = lb.transform(test_labels)\n",
    "    \n",
    "    train_x = train_features.to_numpy()\n",
    "    train_y = train_labels\n",
    "\n",
    "    test_x = test_features.to_numpy()\n",
    "    test_y = test_labels\n",
    "    \n",
    "    train_x = train_x[:,:features]\n",
    "    test_x = test_x[:,:features]\n",
    "    \n",
    "    # normalize data\n",
    "    train_x = preprocessing.normalize(train_x)\n",
    "    test_x = preprocessing.normalize(test_x)\n",
    "    \n",
    "    train_dataset = TimeSeriesDataset(train_x, train_y, seq_len)\n",
    "    test_dataset = TimeSeriesDataset(test_x, test_y, seq_len)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader, test_dataloader\n",
    " \n",
    "    \n",
    "    \n",
    "features = 10\n",
    "batch_size = 30\n",
    "seq_len = 30\n",
    "output_dim = 14\n",
    "\n",
    "train_dataloader, test_dataloader = generate_data(\"S02\",\"Knot_Tying\",features, batch_size, seq_len)\n",
    "\n",
    "for idx,batch in enumerate(train_dataloader):\n",
    "    \n",
    "    print(idx, batch[0].shape ,batch[1].shape ,batch[2].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu117\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def train_loop(dataloader,model,optimizer,criterion, epochs):\n",
    "    \n",
    "    # training loop\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            x, y, y_seq = batch\n",
    "            x = x.to(torch.float32)\n",
    "            y = y.to(torch.float32)\n",
    "            y_seq = y_seq.to(torch.float32)\n",
    "            \n",
    "            y_pred = model(x)\n",
    "\n",
    "            # print(y_pred.shape, y_seq.shape)\n",
    "            \n",
    "            loss = criterion(y_pred, y_seq)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(dataloader):.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_loop(dataloader, model,criterion):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_accuracy = []\n",
    "    for i, batch in enumerate(test_dataloader):\n",
    "        x, y, y_seq = batch\n",
    "        x = x.to(torch.float32)\n",
    "        y = y.to(torch.float32)\n",
    "        y_seq = y_seq.to(torch.float32)\n",
    "        \n",
    "        y_pred = model(x)\n",
    "        \n",
    "        total_inputs = 0\n",
    "        true_pred = []\n",
    "        \n",
    "        \n",
    "        for idx,y in enumerate(y_pred):\n",
    "            \n",
    "            total_inputs += 1\n",
    "            \n",
    "            output_argmax = torch.argmax(y)\n",
    "            gt_argmax = torch.argmax(y_seq[idx])\n",
    "        \n",
    "            if(output_argmax == gt_argmax):\n",
    "                true_pred.append(output_argmax)\n",
    "                \n",
    "            accuracy = len(true_pred)/total_inputs\n",
    "            \n",
    "            # print(\"Accuracy: \",accuracy)\n",
    "            total_accuracy.append(accuracy)\n",
    "            \n",
    "            \n",
    "                \n",
    "        loss = criterion(y_pred, y_seq)\n",
    "        # print(i, \"Loss: \", loss)\n",
    "        \n",
    "    avg_accuracy = np.average(total_accuracy)\n",
    "    print(\"Average accuracy: \", avg_accuracy)\n",
    "    return avg_accuracy\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_model(d_model, nhead, num_layers, features, output_dim, lr ):\n",
    "    model = TransformerModel(input_dim=features, output_dim=output_dim, d_model=d_model, nhead=nhead, num_layers=num_layers)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    return model, optimizer, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = 22\n",
    "batch_size = 30\n",
    "seq_len = 10\n",
    "output_dim = 14\n",
    "\n",
    "task = \"Knot_Tying\"\n",
    "\n",
    "epochs = 1000\n",
    "lr = 1e-4\n",
    "\n",
    "# best config yet: d_model = 64,  nhead=4, num_layers=2\n",
    "d_model = 128\n",
    "nhead=4\n",
    "num_layers=4\n",
    "\n",
    "\n",
    "\n",
    "model,optimizer,criterion = initiate_model(d_model, nhead, num_layers, features, output_dim, lr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Subject Trials:  32\n",
      "Test Subject Trials:  4\n",
      "Epoch 1, Loss: 1.609910\n",
      "Epoch 2, Loss: 1.377104\n",
      "Epoch 3, Loss: 1.226118\n",
      "Epoch 4, Loss: 1.068045\n",
      "Epoch 5, Loss: 0.927260\n",
      "Epoch 6, Loss: 0.849362\n",
      "Epoch 7, Loss: 0.787945\n",
      "Epoch 8, Loss: 0.731561\n",
      "Epoch 9, Loss: 0.690591\n",
      "Epoch 10, Loss: 0.631219\n",
      "Epoch 11, Loss: 0.591877\n",
      "Epoch 12, Loss: 0.494521\n",
      "Epoch 13, Loss: 0.454345\n",
      "Epoch 14, Loss: 0.440464\n"
     ]
    }
   ],
   "source": [
    "subjects = ['S02','S03','S04','S05','S06','S07','S08','S09']\n",
    "# subjects = ['S05']\n",
    "\n",
    "accuracy = []\n",
    "for subject in subjects:\n",
    "    \n",
    "    model,optimizer,criterion = initiate_model(d_model, nhead, num_layers, features, output_dim, lr)\n",
    "\n",
    "    train_dataloader, test_dataloader = generate_data(subject,task,features, batch_size, seq_len)\n",
    "\n",
    "    train_loop(dataloader=train_dataloader, model=model, optimizer=optimizer, criterion=criterion, epochs=epochs)\n",
    "    \n",
    "    acc = evaluation_loop(dataloader=test_dataloader, model=model, criterion=criterion)\n",
    "    \n",
    "    subject_accuracy = {'subject':subject, 'accuracy':acc}\n",
    "    print(subject_accuracy)\n",
    "    accuracy.append(subject_accuracy)\n",
    "\n",
    "    break\n",
    "    \n",
    "\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9599571113075005\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for x in accuracy:\n",
    "    acc.append(x['accuracy'])\n",
    "    \n",
    "print(np.average(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: ./checkpoints//64_4_2_01_pytorch_workflow_model_0.pth\n",
      "done saving!\n"
     ]
    }
   ],
   "source": [
    "# 2. Create model save path \n",
    "MODEL_PATH = \"./checkpoints/\"\n",
    "MODEL_NAME = \"01_pytorch_workflow_model_0.pth\"\n",
    "MODEL_NAME = str(d_model) + \"_\" + str(nhead) + \"_\" + str(num_layers) + \"_\" + MODEL_NAME\n",
    "MODEL_SAVE_PATH = MODEL_PATH +\"/\"+ MODEL_NAME\n",
    "\n",
    "\n",
    "\n",
    "# 3. Save the model state dict \n",
    "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
    "\n",
    "torch.save(obj=model.state_dict(), # only saving the state_dict() only saves the models learned parameters\n",
    "           f=MODEL_SAVE_PATH) \n",
    "\n",
    "\n",
    "print(\"done saving!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy:  0.28562257379625866\n"
     ]
    }
   ],
   "source": [
    "def evaluation_loop():\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_accuracy = []\n",
    "    for i, batch in enumerate(test_dataloader):\n",
    "        x, y, y_seq = batch\n",
    "        x = x.to(torch.float32)\n",
    "        y = y.to(torch.float32)\n",
    "        y_seq = y_seq.to(torch.float32)\n",
    "        \n",
    "        y_pred = model(x)\n",
    "        \n",
    "        total_inputs = 0\n",
    "        true_pred = []\n",
    "        \n",
    "        \n",
    "        for idx,y in enumerate(y_pred):\n",
    "            \n",
    "            total_inputs += 1\n",
    "            \n",
    "            output_argmax = torch.argmax(y)\n",
    "            gt_argmax = torch.argmax(y_seq[idx])\n",
    "        \n",
    "            if(output_argmax == gt_argmax):\n",
    "                true_pred.append(output_argmax)\n",
    "                \n",
    "            accuracy = len(true_pred)/total_inputs\n",
    "            \n",
    "            # print(\"Accuracy: \",accuracy)\n",
    "            total_accuracy.append(accuracy)\n",
    "            \n",
    "            \n",
    "                \n",
    "        loss = criterion(y_pred, y_seq)\n",
    "        # print(i, \"Loss: \", loss)\n",
    "        \n",
    "    print(\"Average accuracy: \", np.average(total_accuracy))\n",
    "    \n",
    "    \n",
    "evaluation_loop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
