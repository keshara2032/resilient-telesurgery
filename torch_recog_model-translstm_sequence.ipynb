{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from torch.optim.lr_scheduler import _LRScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, x_set, y_set, seq_len):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        # return int(np.ceil(len(self.x) / float(self.seq_len)))\n",
    "        return int(np.ceil(len(self.x) ))\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # start_idx = idx * self.seq_len\n",
    "        # end_idx = (idx + 1) * self.seq_len\n",
    "        \n",
    "        # sliding window\n",
    "        start_idx = idx\n",
    "        end_idx = idx + self.seq_len\n",
    "\n",
    "        batch_x = self.x[start_idx:end_idx]\n",
    "        batch_y = self.y[start_idx:end_idx]\n",
    "\n",
    "        # Count the occurrences of each row\n",
    "        unique_rows, counts = np.unique(batch_y, axis=0, return_counts=True)\n",
    "        \n",
    "        # print(\"index:\",idx,\"ybatch:\",batch_y)\n",
    "\n",
    "        # Get the index of the row with the highest count\n",
    "        most_common_row_index = np.argmax(counts)\n",
    "\n",
    "        # Get the most common row\n",
    "        most_common_row = unique_rows[most_common_row_index]\n",
    "        \n",
    "        \n",
    "        batch_y_bin = most_common_row\n",
    "\n",
    "        # Convert NumPy arrays to PyTorch tensors\n",
    "        batch_x = torch.from_numpy(batch_x)\n",
    "        batch_y = torch.from_numpy(batch_y)\n",
    "        batch_y_bin = torch.from_numpy(batch_y_bin)\n",
    "\n",
    "        # Pad sequences to ensure they have the same length within the batch\n",
    "        pad_len = self.seq_len - batch_x.shape[0]\n",
    "        if pad_len > 0:\n",
    "            pad_shape = (pad_len,) + batch_x.shape[1:]\n",
    "            pad_shape_y = (pad_len,) + batch_y.shape[1:]\n",
    "\n",
    "            batch_x = torch.cat([batch_x, torch.zeros(pad_shape)], dim=0)\n",
    "            batch_y = torch.cat([batch_y, torch.zeros(pad_shape_y)], dim=0)\n",
    "\n",
    "        return batch_x, batch_y, batch_y_bin\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        indices = np.arange(len(self.x))\n",
    "        np.random.shuffle(indices)\n",
    "        self.x = self.x[indices]\n",
    "        self.y = self.y[indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Subject Trials:  28\n",
      "Test Subject Trials:  4\n",
      "0 torch.Size([30, 30, 10]) torch.Size([30, 30, 14]) torch.Size([30, 14])\n"
     ]
    }
   ],
   "source": [
    "def generate_data(subject_id, task, features, batch_size, seq_len):    \n",
    "    \n",
    "    csv_path = './ProcessedDatasets/' + task\n",
    "    # csv_path = './Dataset'\n",
    "    \n",
    "    csv_files = glob.glob(csv_path + \"/*.csv\")\n",
    "    \n",
    "    train_df_list = []\n",
    "    test_df_list = []\n",
    "    \n",
    "    for file in csv_files:\n",
    "        if(subject_id in file):\n",
    "            test_df_list.append(pd.read_csv(file))\n",
    "#             print(file)\n",
    "        else:\n",
    "            train_df_list.append(pd.read_csv(file))\n",
    "            \n",
    "\n",
    "    print('Train Subject Trials: ',len(train_df_list))\n",
    "    print('Test Subject Trials: ',len(test_df_list))\n",
    "    \n",
    "    # Concatenate all DataFrames\n",
    "    train_df   = pd.concat(train_df_list, ignore_index=True)\n",
    "    test_df   = pd.concat(test_df_list, ignore_index=True)\n",
    "\n",
    "    \n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "\n",
    "    train_labels= train_df.pop('label')\n",
    "    train_features = train_df\n",
    "\n",
    "    test_labels= test_df.pop('label')\n",
    "    test_features = test_df\n",
    "\n",
    "\n",
    "    all_class_names = [\"G1\", 'G2', 'G3', 'G4', 'G5', 'G6', 'G8', 'G9', 'G10', 'G11', 'G12', 'G13', 'G14', 'G15']\n",
    "    lb.fit(all_class_names)\n",
    "\n",
    "    train_labels = lb.transform(train_labels)\n",
    "    test_labels = lb.transform(test_labels)\n",
    "    \n",
    "    train_x = train_features.to_numpy()\n",
    "    train_y = train_labels\n",
    "\n",
    "    test_x = test_features.to_numpy()\n",
    "    test_y = test_labels\n",
    "    \n",
    "    train_x = train_x[:,:features]\n",
    "    test_x = test_x[:,:features]\n",
    "    \n",
    "\n",
    "    train_dataset = TimeSeriesDataset(train_x, train_y, seq_len)\n",
    "    test_dataset = TimeSeriesDataset(test_x, test_y, seq_len)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader, test_dataloader\n",
    " \n",
    "    \n",
    "    \n",
    "features = 10\n",
    "batch_size = 30\n",
    "seq_len = 30\n",
    "output_dim = 14\n",
    "\n",
    "train_dataloader, test_dataloader = generate_data(\"S02\",\"Knot_Tying\",features, batch_size, seq_len)\n",
    "\n",
    "for idx,batch in enumerate(train_dataloader):\n",
    "    \n",
    "    print(idx, batch[0].shape ,batch[1].shape ,batch[2].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu117\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def train_loop(dataloader,model,optimizer,scheduler,criterion, epochs):\n",
    "    \n",
    "    # training loop\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, batch in enumerate(dataloader):\n",
    "\n",
    "            \n",
    "            x, y, y_seq = batch\n",
    "            x = x.to(torch.float32)\n",
    "            y = y.to(torch.float32)\n",
    "            y_seq = y_seq.to(torch.float32)\n",
    "            \n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            y_seq = y_seq.cuda()\n",
    "            \n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            y_pred = model(x)\n",
    "\n",
    "            # print(y_pred.shape, y_seq.shape)\n",
    "            \n",
    "            loss = criterion(y_pred, y_seq)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(dataloader):.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_loop(dataloader, model,criterion):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_accuracy = []\n",
    "    for i, batch in enumerate(test_dataloader):\n",
    "        x, y, y_seq = batch\n",
    "        x = x.to(torch.float32)\n",
    "        y = y.to(torch.float32)\n",
    "        y_seq = y_seq.to(torch.float32)\n",
    "        \n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        y_seq = y_seq.cuda()\n",
    "        \n",
    "        y_pred = model(x)\n",
    "        \n",
    "        total_inputs = 0\n",
    "        true_pred = []\n",
    "        \n",
    "        \n",
    "        for idx,y in enumerate(y_pred):\n",
    "            \n",
    "            total_inputs += 1\n",
    "            \n",
    "            output_argmax = torch.argmax(y)\n",
    "            gt_argmax = torch.argmax(y_seq[idx])\n",
    "        \n",
    "            if(output_argmax == gt_argmax):\n",
    "                true_pred.append(output_argmax)\n",
    "                \n",
    "            accuracy = len(true_pred)/total_inputs\n",
    "            \n",
    "            # print(\"Accuracy: \",accuracy)\n",
    "            total_accuracy.append(accuracy)\n",
    "            \n",
    "            \n",
    "                \n",
    "        loss = criterion(y_pred, y_seq)\n",
    "        # print(i, \"Loss: \", loss)\n",
    "        \n",
    "    avg_accuracy = np.average(total_accuracy)\n",
    "    print(\"Average accuracy: \", avg_accuracy)\n",
    "    return avg_accuracy\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CyclicLR(_LRScheduler):\n",
    "    \n",
    "    def __init__(self, optimizer, schedule, last_epoch=-1):\n",
    "        assert callable(schedule)\n",
    "        self.schedule = schedule\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [self.schedule(self.last_epoch, lr) for lr in self.base_lrs]\n",
    "\n",
    "\n",
    "\n",
    "def cosine(t_max, eta_min=0):\n",
    "    \n",
    "    def scheduler(epoch, base_lr):\n",
    "        t = epoch % t_max\n",
    "        return eta_min + (base_lr - eta_min)*(1 + np.cos(np.pi*t/t_max))/2\n",
    "    \n",
    "    return scheduler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer+LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8])\n"
     ]
    }
   ],
   "source": [
    "# target output size of 5\n",
    "\n",
    "class GlobalMaxPooling1D(nn.Module):\n",
    "\n",
    "    def __init__(self, data_format='channels_last'):\n",
    "        super(GlobalMaxPooling1D, self).__init__()\n",
    "        self.data_format = data_format\n",
    "        self.step_axis = 1 if self.data_format == 'channels_last' else 2\n",
    "\n",
    "    def forward(self, input):\n",
    "        return torch.max(input, axis=self.step_axis).values\n",
    "    \n",
    "\n",
    "m = GlobalMaxPooling1D()\n",
    "input = torch.randn(1, 64, 8)\n",
    "output = m(input)\n",
    "print(output.shape)\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, d_model, nhead, num_layers, hidden_dim, layer_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout), num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        self.lstm = LSTMModel(d_model, hidden_dim, layer_dim, output_dim)\n",
    "        \n",
    "        self.max_pool = GlobalMaxPooling1D()\n",
    "        self.fc = nn.Linear(input_dim, d_model)\n",
    "        self.out = nn.Linear(d_model, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        x = self.max_pool(x)\n",
    "  \n",
    "        x = self.transformer(x)\n",
    "    \n",
    "        x = self.lstm(x)\n",
    "        \n",
    "#         x = self.out(x)\n",
    "\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_model(d_model, nhead, num_layers,input_dim,output_dim, hidden_dim, layer_dim, lr, iterations_per_epoch ):\n",
    "\n",
    "    model = TransformerModel(input_dim=input_dim, output_dim=output_dim, d_model=d_model, nhead=nhead, num_layers=num_layers, hidden_dim=hidden_dim, layer_dim=layer_dim)\n",
    "\n",
    "    \n",
    "    \n",
    "    model = model.cuda()\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr) # adam\n",
    "    \n",
    "    # optimizer = torch.optim.RMSprop(model.parameters(), lr=lr) # custom\n",
    "    \n",
    "    sched = CyclicLR(optimizer, cosine(t_max=iterations_per_epoch * 2, eta_min=lr/100))\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    \n",
    "    \n",
    "    return model, optimizer,sched, criterion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "\n",
    "lr = 0.0001\n",
    "\n",
    "features = 36\n",
    "batch_size = 64\n",
    "seq_len = 10\n",
    "\n",
    "input_dim = features  \n",
    "output_dim = 14\n",
    "\n",
    "# lstm\n",
    "hidden_dim = 256\n",
    "layer_dim = 3\n",
    "seq_dim = 128\n",
    "\n",
    "#transformer\n",
    "d_model = 64\n",
    "nhead=8\n",
    "num_layers=2\n",
    "\n",
    "task = \"Knot_Tying\"\n",
    "\n",
    "epochs = 50\n",
    "# iterations_per_epoch = len(train_dataloader)\n",
    "iterations_per_epoch = 500\n",
    "\n",
    "model,optimizer,scheduler,criterion = initiate_model(d_model, nhead, num_layers,input_dim,output_dim, hidden_dim, layer_dim, lr,  iterations_per_epoch=iterations_per_epoch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Subject Trials:  28\n",
      "Test Subject Trials:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kesharaw/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.703561\n",
      "Epoch 2, Loss: 1.603368\n",
      "Epoch 3, Loss: 1.592036\n",
      "Epoch 4, Loss: 1.590782\n",
      "Epoch 5, Loss: 1.589524\n",
      "Epoch 6, Loss: 1.586355\n",
      "Epoch 7, Loss: 1.486991\n",
      "Epoch 8, Loss: 1.458595\n",
      "Epoch 9, Loss: 1.425348\n",
      "Epoch 10, Loss: 1.402700\n",
      "Epoch 11, Loss: 1.416410\n",
      "Epoch 12, Loss: 1.386588\n",
      "Epoch 13, Loss: 1.375205\n",
      "Epoch 14, Loss: 1.361646\n",
      "Epoch 15, Loss: 1.443935\n",
      "Epoch 16, Loss: 1.298825\n",
      "Epoch 17, Loss: 1.329703\n",
      "Epoch 18, Loss: 1.578556\n",
      "Epoch 19, Loss: 1.497748\n",
      "Epoch 20, Loss: 1.423135\n",
      "Epoch 21, Loss: 1.372320\n",
      "Epoch 22, Loss: 1.343771\n",
      "Epoch 23, Loss: 1.208901\n",
      "Epoch 24, Loss: 1.170129\n",
      "Epoch 25, Loss: 1.156703\n",
      "Epoch 26, Loss: 1.119967\n",
      "Epoch 27, Loss: 1.069628\n",
      "Epoch 28, Loss: 1.019884\n",
      "Epoch 29, Loss: 1.039026\n",
      "Epoch 30, Loss: 0.976380\n",
      "Epoch 31, Loss: 0.947500\n",
      "Epoch 32, Loss: 0.950489\n",
      "Epoch 33, Loss: 0.940350\n",
      "Epoch 34, Loss: 0.894078\n",
      "Epoch 35, Loss: 0.872550\n",
      "Epoch 36, Loss: 0.923983\n",
      "Epoch 37, Loss: 0.865996\n",
      "Epoch 38, Loss: 0.817318\n",
      "Epoch 39, Loss: 0.842469\n",
      "Epoch 40, Loss: 0.835061\n",
      "Epoch 41, Loss: 0.777099\n",
      "Epoch 42, Loss: 0.815201\n",
      "Epoch 43, Loss: 0.827120\n",
      "Epoch 44, Loss: 0.780893\n",
      "Epoch 45, Loss: 0.746745\n",
      "Epoch 46, Loss: 0.745098\n",
      "Epoch 47, Loss: 0.732780\n",
      "Epoch 48, Loss: 0.668343\n",
      "Epoch 49, Loss: 0.737191\n",
      "Epoch 50, Loss: 0.708850\n",
      "Average accuracy:  0.46821887925146555\n",
      "{'subject': 'S02', 'accuracy': 0.46821887925146555}\n",
      "[{'subject': 'S02', 'accuracy': 0.46821887925146555}]\n"
     ]
    }
   ],
   "source": [
    "subjects = ['S02','S03','S04','S05','S06','S07','S08','S09']\n",
    "subjects = ['S02']\n",
    "\n",
    "accuracy = []\n",
    "for subject in subjects:\n",
    "    \n",
    "#     model,optimizer,scheduler,criterion = initiate_model(d_model, nhead, num_layers,input_dim,output_dim, hidden_dim, layer_dim, lr,  iterations_per_epoch=iterations_per_epoch)\n",
    "\n",
    "    train_dataloader, test_dataloader = generate_data(subject,task,features, batch_size, seq_len)\n",
    "\n",
    "    train_loop(dataloader=train_dataloader, model=model, optimizer=optimizer, scheduler=scheduler, criterion=criterion, epochs=epochs)\n",
    "    \n",
    "    acc = evaluation_loop(dataloader=test_dataloader, model=model, criterion=criterion)\n",
    "    \n",
    "    subject_accuracy = {'subject':subject, 'accuracy':acc}\n",
    "    print(subject_accuracy)\n",
    "    accuracy.append(subject_accuracy)\n",
    "    \n",
    "\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m acc \u001b[39m=\u001b[39m []\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m accuracy:\n\u001b[1;32m      3\u001b[0m     acc\u001b[39m.\u001b[39mappend(x[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(np\u001b[39m.\u001b[39maverage(acc))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for x in accuracy:\n",
    "    acc.append(x['accuracy'])\n",
    "    \n",
    "print(np.average(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: ./checkpoints//64_4_2_01_pytorch_workflow_model_0.pth\n",
      "done saving!\n"
     ]
    }
   ],
   "source": [
    "# 2. Create model save path \n",
    "MODEL_PATH = \"./checkpoints/\"\n",
    "MODEL_NAME = \"01_pytorch_workflow_model_0.pth\"\n",
    "MODEL_NAME = str(d_model) + \"_\" + str(nhead) + \"_\" + str(num_layers) + \"_\" + MODEL_NAME\n",
    "MODEL_SAVE_PATH = MODEL_PATH +\"/\"+ MODEL_NAME\n",
    "\n",
    "\n",
    "\n",
    "# 3. Save the model state dict \n",
    "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
    "\n",
    "torch.save(obj=model.state_dict(), # only saving the state_dict() only saves the models learned parameters\n",
    "           f=MODEL_SAVE_PATH) \n",
    "\n",
    "\n",
    "print(\"done saving!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy:  0.28562257379625866\n"
     ]
    }
   ],
   "source": [
    "def evaluation_loop():\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_accuracy = []\n",
    "    for i, batch in enumerate(test_dataloader):\n",
    "        x, y, y_seq = batch\n",
    "        x = x.to(torch.float32)\n",
    "        y = y.to(torch.float32)\n",
    "        y_seq = y_seq.to(torch.float32)\n",
    "        \n",
    "        y_pred = model(x)\n",
    "        \n",
    "        total_inputs = 0\n",
    "        true_pred = []\n",
    "        \n",
    "        \n",
    "        for idx,y in enumerate(y_pred):\n",
    "            \n",
    "            total_inputs += 1\n",
    "            \n",
    "            output_argmax = torch.argmax(y)\n",
    "            gt_argmax = torch.argmax(y_seq[idx])\n",
    "        \n",
    "            if(output_argmax == gt_argmax):\n",
    "                true_pred.append(output_argmax)\n",
    "                \n",
    "            accuracy = len(true_pred)/total_inputs\n",
    "            \n",
    "            # print(\"Accuracy: \",accuracy)\n",
    "            total_accuracy.append(accuracy)\n",
    "            \n",
    "            \n",
    "                \n",
    "        loss = criterion(y_pred, y_seq)\n",
    "        # print(i, \"Loss: \", loss)\n",
    "        \n",
    "    print(\"Average accuracy: \", np.average(total_accuracy))\n",
    "    \n",
    "    \n",
    "evaluation_loop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
