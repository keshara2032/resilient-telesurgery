{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import json\n",
    "\n",
    "from typing import List\n",
    "import os\n",
    "from functools import partial\n",
    "import torch.nn.functional as F\n",
    "from timeit import default_timer as timer\n",
    "from utils import get_dataloaders\n",
    "\n",
    "from models.utils import *\n",
    "from datagen import kinematic_feature_names, kinematic_feature_names_jigsaws, class_names, all_class_names, state_variables\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset & Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3155, 3266, 3048, 2449, 2527, 2653, 2002, 2303, 2642, 1758, 3583, 2925, 2516, 2869, 2482, 2803, 2374, 2235, 2372, 2028, 9011, 3378, 5160, 2620, 2453, 5274, 3320, 4020, 3279, 4310, 4411, 4145, 3730, 3700]\n",
      "['G1' 'G10' 'G11' 'G2' 'G3' 'G4' 'G5' 'G6' 'G8' 'G9']\n",
      "[0 0 0 ... 2 2 2]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]]\n",
      "[5555, 3420, 3267, 3256, 2847]\n",
      "['G1' 'G10' 'G11' 'G2' 'G3' 'G4' 'G5' 'G6' 'G8' 'G9']\n",
      "[0 0 0 ... 2 2 2]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]]\n",
      "datasets lengths:  11078 1832\n",
      "X shape:  (110801, 43) (18345, 43)\n",
      "Y shape:  (110801, 10) (18345, 10)\n",
      "Obs Kinematics Shape:  (10, 43)\n",
      "Obs Target Shape:  (11, 10)\n",
      "Future Target Shape:  (10, 10)\n",
      "Future Kinematics Shape:  (10, 43)\n",
      "Train N Trials:  34\n",
      "Train Max Length:  9011\n",
      "Test N Trials:  5\n",
      "Test Max Length:  5555\n",
      "Features:  ['PSML_position_x', 'PSML_position_y', 'PSML_position_z', 'PSML_rotation_0', 'PSML_rotation_1', 'PSML_rotation_2', 'PSML_rotation_3', 'PSML_rotation_4', 'PSML_rotation_5', 'PSML_rotation_6', 'PSML_rotation_7', 'PSML_rotation_8', 'PSML_velocity_x', 'PSML_velocity_y', 'PSML_velocity_z', 'PSML_velocity_rot0', 'PSML_velocity_rot1', 'PSML_velocity_rot2', 'PSML_gripper_angle', 'PSMR_position_x', 'PSMR_position_y', 'PSMR_position_z', 'PSMR_rotation_0', 'PSMR_rotation_1', 'PSMR_rotation_2', 'PSMR_rotation_3', 'PSMR_rotation_4', 'PSMR_rotation_5', 'PSMR_rotation_6', 'PSMR_rotation_7', 'PSMR_rotation_8', 'PSMR_velocity_x', 'PSMR_velocity_y', 'PSMR_velocity_z', 'PSMR_velocity_rot0', 'PSMR_velocity_rot1', 'PSMR_velocity_rot2', 'PSMR_gripper_angle', 'left_holding', 'left_contact', 'right_holding', 'right_contact', 'needle_state']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### -------------------------- DATA -----------------------------------------------------\n",
    "tasks = [\"Suturing\"]\n",
    "Features = kinematic_feature_names_jigsaws[38:] + state_variables #kinematic features + state variable features\n",
    "\n",
    "one_hot = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "observation_window = 10\n",
    "prediction_window = 10\n",
    "batch_size = 64\n",
    "\n",
    "user_left_out = 2\n",
    "cast = True\n",
    "include_image_features = False\n",
    "normalizer = '' # ('standardization', 'min-max', 'power', '')\n",
    "step = 1 # 1 - 30 Hz\n",
    "\n",
    "train_dataloader, valid_dataloader = get_dataloaders(tasks,\n",
    "                                                     user_left_out,\n",
    "                                                     observation_window,\n",
    "                                                     prediction_window,\n",
    "                                                     batch_size,\n",
    "                                                     one_hot,\n",
    "                                                     class_names = class_names['Suturing'],\n",
    "                                                     feature_names = Features,\n",
    "                                                     include_image_features=include_image_features,\n",
    "                                                     cast = cast,\n",
    "                                                     normalizer = normalizer,\n",
    "                                                     step=step)\n",
    "\n",
    "print(\"datasets lengths: \", len(train_dataloader.dataset), len(valid_dataloader.dataset))\n",
    "print(\"X shape: \", train_dataloader.dataset.X.shape, valid_dataloader.dataset.X.shape)\n",
    "print(\"Y shape: \", train_dataloader.dataset.Y.shape, valid_dataloader.dataset.Y.shape)\n",
    "\n",
    "# loader generator aragement: (src, tgt, future_gesture, future_kinematics)\n",
    "print(\"Obs Kinematics Shape: \", train_dataloader.dataset[0][0].shape) \n",
    "print(\"Obs Target Shape: \", train_dataloader.dataset[0][1].shape)\n",
    "print(\"Future Target Shape: \", train_dataloader.dataset[0][2].shape)\n",
    "print(\"Future Kinematics Shape: \", train_dataloader.dataset[0][3].shape)\n",
    "print(\"Train N Trials: \", train_dataloader.dataset.get_num_trials())\n",
    "print(\"Train Max Length: \", train_dataloader.dataset.get_max_len())\n",
    "print(\"Test N Trials: \", valid_dataloader.dataset.get_num_trials())\n",
    "print(\"Test Max Length: \", valid_dataloader.dataset.get_max_len())\n",
    "print(\"Features: \", train_dataloader.dataset.get_feature_names())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10, 14])\n",
      "torch.Size([64, 14])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example tensor of shape [batch_size, seq_len, features]\n",
    "tensor = torch.tensor([[[1, 2, 3], [3, 2, 1], [1, 1, 2]],\n",
    "                       [[2, 3, 3], [1, 2, 2], [3, 2, 3]]])\n",
    "tensor = torch.zeros(64,10,14)\n",
    "print(tensor.shape)\n",
    "# Calculate the most common feature along the seq_len axis for each batch\n",
    "most_common_feature = torch.mode(tensor, dim=1).values\n",
    "\n",
    "print(most_common_feature.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mostcommon(tensor,device):\n",
    "    # # 64,10,10\n",
    "    # # Count the occurrences of each row\n",
    "    # unique_rows, counts = np.unique(batch_y, axis=0, return_counts=True)\n",
    "\n",
    "    # # Get the index of the row with the highest count\n",
    "    # most_common_row_index = np.argmax(counts)\n",
    "\n",
    "    # # Get the most common row\n",
    "    # most_common_row = unique_rows[most_common_row_index]\n",
    "    \n",
    "    # batch_y_bin = most_common_row\n",
    "    \n",
    "    # batch_y_bin = torch.from_numpy(batch_y_bin)\n",
    "    \n",
    "    batch_y_bin = torch.mode(tensor, dim=1).values\n",
    "    batch_y_bin = batch_y_bin.to(device)\n",
    "    \n",
    "    return batch_y_bin\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out, _ = self.lstm(x)\n",
    "\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CyclicLR(_LRScheduler):\n",
    "    \n",
    "    def __init__(self, optimizer, schedule, last_epoch=-1):\n",
    "        assert callable(schedule)\n",
    "        self.schedule = schedule\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [self.schedule(self.last_epoch, lr) for lr in self.base_lrs]\n",
    "\n",
    "\n",
    "\n",
    "def cosine(t_max, eta_min=0):\n",
    "    \n",
    "    def scheduler(epoch, base_lr):\n",
    "        t = epoch % t_max\n",
    "        return eta_min + (base_lr - eta_min)*(1 + np.cos(np.pi*t/t_max))/2\n",
    "    \n",
    "    return scheduler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer+LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8])\n"
     ]
    }
   ],
   "source": [
    "# target output size of 5\n",
    "\n",
    "class GlobalMaxPooling1D(nn.Module):\n",
    "\n",
    "    def __init__(self, data_format='channels_last'):\n",
    "        super(GlobalMaxPooling1D, self).__init__()\n",
    "        self.data_format = data_format\n",
    "        self.step_axis = 1 if self.data_format == 'channels_last' else 2\n",
    "\n",
    "    def forward(self, input):\n",
    "        return torch.max(input, axis=self.step_axis).values\n",
    "    \n",
    "\n",
    "m = GlobalMaxPooling1D()\n",
    "input = torch.randn(1, 64, 8)\n",
    "output = m(input)\n",
    "print(output.shape)\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, d_model, nhead, num_layers, hidden_dim, layer_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout), num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        self.lstm = LSTMModel(d_model, hidden_dim, layer_dim, output_dim)\n",
    "        \n",
    "        self.max_pool = GlobalMaxPooling1D()\n",
    "        self.fc = nn.Linear(input_dim, d_model)\n",
    "        self.out = nn.Linear(d_model, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        \n",
    "        x = self.max_pool(x)\n",
    "        \n",
    "        x = self.transformer(x)\n",
    "    \n",
    "        x = self.lstm(x)\n",
    "        \n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_model(d_model, nhead, num_layers,input_dim,output_dim, hidden_dim, layer_dim, lr, iterations_per_epoch ):\n",
    "\n",
    "    model = TransformerModel(input_dim=input_dim, output_dim=output_dim, d_model=d_model, nhead=nhead, num_layers=num_layers, hidden_dim=hidden_dim, layer_dim=layer_dim)\n",
    "\n",
    "    \n",
    "    \n",
    "    model = model.cuda()\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr) # adam\n",
    "    \n",
    "    # optimizer = torch.optim.RMSprop(model.parameters(), lr=lr) # custom\n",
    "    \n",
    "    sched = CyclicLR(optimizer, cosine(t_max=iterations_per_epoch * 2, eta_min=lr/100))\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    return model, optimizer,sched, criterion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loop(model, test_dataloader, criterion):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # eval\n",
    "        losses = []\n",
    "        ypreds, gts = [], []\n",
    "        \n",
    "        for src, tgt, future_gesture, future_kinematics in test_dataloader:\n",
    "            src = src\n",
    "            y = find_mostcommon(tgt[:,1:,:], device)\n",
    "            \n",
    "            y_pred = model(src) # [64,10]\n",
    "        \n",
    "            pred = torch.argmax(y_pred,dim=1)\n",
    "            gt = torch.argmax(y,dim=1)\n",
    "            \n",
    "            pred = pred.cpu().numpy()\n",
    "            gt = gt.cpu().numpy()\n",
    "            \n",
    "            ypreds.append(pred)\n",
    "            gts.append(gt)\n",
    "            \n",
    "            loss = criterion(y_pred, y)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "          \n",
    "        ypreds = np.concatenate(ypreds)  \n",
    "        gts = np.concatenate(gts)  \n",
    "        \n",
    "        get_classification_report(ypreds,gts,test_dataloader.dataset.get_target_names())\n",
    "            \n",
    "        # Compare each element and count matches\n",
    "        matches = np.sum(ypreds == gts)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = matches / len(ypreds)\n",
    "\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        \n",
    "        return np.mean(losses), ypreds, gts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def traintest_loop(train_dataloader,test_dataloader,model,optimizer,scheduler,criterion, epochs):\n",
    "    \n",
    "    # training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for bi, (src, tgt, future_gesture, future_kinematics) in enumerate(tqdm(train_dataloader)):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            src = src\n",
    "            y = find_mostcommon(tgt[:,1:,:], device)\n",
    "            \n",
    "            y_pred = model(src) # [64,10]\n",
    "            # print('prediction,gt:',y_pred.shape, y.shape, tgt.shape)\n",
    "            \n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        print(f\"Training Epoch {epoch+1}, Loss: {running_loss / len(train_dataloader):.6f}\")\n",
    "        \n",
    "        # evaluation loop\n",
    "        val_loss, ypreds, gts = eval_loop(model,test_dataloader,criterion)\n",
    "        print(f\"Valdiation Epoch {epoch+1}, Loss: {val_loss:.6f}\")\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 10\n"
     ]
    }
   ],
   "source": [
    "        \n",
    "\n",
    "lr = 0.00001\n",
    "\n",
    "batch_size = 16\n",
    "seq_len = 10\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "features = batch[0].shape[-1]\n",
    "output_dim = batch[2].shape[-1]\n",
    "\n",
    "input_dim = features  \n",
    "\n",
    "# lstm\n",
    "hidden_dim = 512\n",
    "layer_dim = 2\n",
    "seq_dim = 128\n",
    "\n",
    "#transformer\n",
    "d_model = 512\n",
    "nhead=4\n",
    "num_layers=4\n",
    "\n",
    "task = \"Suturing\"\n",
    "\n",
    "epochs = 10\n",
    "# iterations_per_epoch = len(train_dataloader)\n",
    "iterations_per_epoch = 500\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "features = batch[0].shape[-1]\n",
    "output_dim = batch[1].shape[-1]\n",
    "\n",
    "print(features, output_dim)\n",
    "\n",
    "model,optimizer,scheduler,criterion = initiate_model(d_model, nhead, num_layers,input_dim,output_dim, hidden_dim, layer_dim, lr,  iterations_per_epoch=iterations_per_epoch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3155, 3266, 3048, 2449, 2527, 2653, 2002, 2303, 2642, 1758, 3583, 2925, 2516, 2869, 2482, 2803, 2374, 2235, 2372, 2028, 9011, 3378, 5160, 2620, 2453, 5274, 3320, 4020, 3279, 4310, 4411, 4145, 3730, 3700]\n",
      "['G1' 'G10' 'G11' 'G2' 'G3' 'G4' 'G5' 'G6' 'G8' 'G9']\n",
      "[0 0 0 ... 2 2 2]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]]\n",
      "[5555, 3420, 3267, 3256, 2847]\n",
      "['G1' 'G10' 'G11' 'G2' 'G3' 'G4' 'G5' 'G6' 'G8' 'G9']\n",
      "[0 0 0 ... 2 2 2]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 693/693 [00:04<00:00, 164.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1, Loss: 1.821988\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 29\u001b[0m\n\u001b[1;32m     12\u001b[0m     user_left_out \u001b[39m=\u001b[39m subject\n\u001b[1;32m     14\u001b[0m     train_dataloader, valid_dataloader \u001b[39m=\u001b[39m get_dataloaders(tasks,\n\u001b[1;32m     15\u001b[0m                                                         user_left_out,\n\u001b[1;32m     16\u001b[0m                                                         observation_window,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m                                                         normalizer \u001b[39m=\u001b[39m normalizer,\n\u001b[1;32m     25\u001b[0m                                                         step\u001b[39m=\u001b[39mstep)\n\u001b[0;32m---> 29\u001b[0m     traintest_loop(train_dataloader,valid_dataloader,model,optimizer,scheduler,criterion, epochs)\n\u001b[1;32m     32\u001b[0m     \u001b[39m# Serializing json\u001b[39;00m\n\u001b[1;32m     33\u001b[0m json_object \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mdumps(accuracy, indent\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 28\u001b[0m, in \u001b[0;36mtraintest_loop\u001b[0;34m(train_dataloader, test_dataloader, model, optimizer, scheduler, criterion, epochs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining Epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mrunning_loss\u001b[39m \u001b[39m\u001b[39m/\u001b[39m\u001b[39m \u001b[39m\u001b[39mlen\u001b[39m(train_dataloader)\u001b[39m:\u001b[39;00m\u001b[39m.6f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[39m# evaluation loop\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m val_loss, ypreds, gts \u001b[39m=\u001b[39m eval_loop(model,test_dataloader,criterion)\n\u001b[1;32m     29\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mValdiation Epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mval_loss\u001b[39m:\u001b[39;00m\u001b[39m.6f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m, in \u001b[0;36meval_loop\u001b[0;34m(model, test_dataloader, criterion)\u001b[0m\n\u001b[1;32m      9\u001b[0m src \u001b[39m=\u001b[39m src\n\u001b[1;32m     10\u001b[0m y \u001b[39m=\u001b[39m find_mostcommon(tgt[:,\u001b[39m1\u001b[39m:,:], device)\n\u001b[0;32m---> 12\u001b[0m y_pred \u001b[39m=\u001b[39m model(src) \u001b[39m# [64,10]\u001b[39;00m\n\u001b[1;32m     14\u001b[0m pred \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(y_pred,dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     15\u001b[0m gt \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(y,dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[7], line 42\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     37\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(x)\n\u001b[1;32m     40\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_pool(x)\n\u001b[0;32m---> 42\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(x)\n\u001b[1;32m     44\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlstm(x)\n\u001b[1;32m     46\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/transformer.py:315\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    312\u001b[0m is_causal \u001b[39m=\u001b[39m make_causal\n\u001b[1;32m    314\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m--> 315\u001b[0m     output \u001b[39m=\u001b[39m mod(output, src_mask\u001b[39m=\u001b[39;49mmask, is_causal\u001b[39m=\u001b[39;49mis_causal, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask_for_layers)\n\u001b[1;32m    317\u001b[0m \u001b[39mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    318\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mto_padded_tensor(\u001b[39m0.\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/transformer.py:591\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    589\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x))\n\u001b[1;32m    590\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 591\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sa_block(x, src_mask, src_key_padding_mask, is_causal\u001b[39m=\u001b[39;49mis_causal))\n\u001b[1;32m    592\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(x))\n\u001b[1;32m    594\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/transformer.py:599\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sa_block\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor,\n\u001b[1;32m    598\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 599\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(x, x, x,\n\u001b[1;32m    600\u001b[0m                        attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m    601\u001b[0m                        key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[1;32m    602\u001b[0m                        need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, is_causal\u001b[39m=\u001b[39;49mis_causal)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    603\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout1(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/activation.py:1205\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1191\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1192\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[1;32m   1193\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1202\u001b[0m         average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1203\u001b[0m         is_causal\u001b[39m=\u001b[39mis_causal)\n\u001b[1;32m   1204\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1205\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[1;32m   1206\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[1;32m   1207\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[1;32m   1208\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[1;32m   1209\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m   1210\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[1;32m   1211\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[1;32m   1212\u001b[0m         need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[1;32m   1213\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m   1214\u001b[0m         average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights,\n\u001b[1;32m   1215\u001b[0m         is_causal\u001b[39m=\u001b[39;49mis_causal)\n\u001b[1;32m   1216\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[1;32m   1217\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:5224\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5222\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m use_separate_proj_weight:\n\u001b[1;32m   5223\u001b[0m     \u001b[39massert\u001b[39;00m in_proj_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39muse_separate_proj_weight is False but in_proj_weight is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 5224\u001b[0m     q, k, v \u001b[39m=\u001b[39m _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)\n\u001b[1;32m   5225\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   5226\u001b[0m     \u001b[39massert\u001b[39;00m q_proj_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39muse_separate_proj_weight is True but q_proj_weight is None\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:4787\u001b[0m, in \u001b[0;36m_in_projection_packed\u001b[0;34m(q, k, v, w, b)\u001b[0m\n\u001b[1;32m   4785\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   4786\u001b[0m     b_q, b_k, b_v \u001b[39m=\u001b[39m b\u001b[39m.\u001b[39mchunk(\u001b[39m3\u001b[39m)\n\u001b[0;32m-> 4787\u001b[0m \u001b[39mreturn\u001b[39;00m linear(q, w_q, b_q), linear(k, w_k, b_k), linear(v, w_v, b_v)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "subjects = [2,3,4,5,6,7,8,9]\n",
    "subjects = [2]\n",
    "\n",
    "\n",
    "with open(\"results.json\", \"w\") as outfile:\n",
    "    \n",
    "    accuracy = []\n",
    "    for subject in (subjects):\n",
    "\n",
    "        model,optimizer,scheduler,criterion = initiate_model(d_model, nhead, num_layers,input_dim,output_dim, hidden_dim, layer_dim, lr,  iterations_per_epoch=iterations_per_epoch)\n",
    "\n",
    "        user_left_out = subject\n",
    "\n",
    "        train_dataloader, valid_dataloader = get_dataloaders(tasks,\n",
    "                                                            user_left_out,\n",
    "                                                            observation_window,\n",
    "                                                            prediction_window,\n",
    "                                                            batch_size,\n",
    "                                                            one_hot,\n",
    "                                                            class_names = class_names['Suturing'],\n",
    "                                                            feature_names = Features,\n",
    "                                                            include_image_features=include_image_features,\n",
    "                                                            cast = cast,\n",
    "                                                            normalizer = normalizer,\n",
    "                                                            step=step)\n",
    "\n",
    "\n",
    "\n",
    "        traintest_loop(train_dataloader,valid_dataloader,model,optimizer,scheduler,criterion, epochs)\n",
    "\n",
    "\n",
    "        # Serializing json\n",
    "    json_object = json.dumps(accuracy, indent=4)\n",
    "    outfile.write(json_object)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6777466186830305\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for x in accuracy:\n",
    "    acc.append(x['accuracy'])\n",
    "    \n",
    "print(np.average(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: ./checkpoints//transformer_encoder__lstm_01_pytorch_workflow_model_0.pth\n",
      "done saving!\n"
     ]
    }
   ],
   "source": [
    "# 2. Create model save path \n",
    "MODEL_PATH = \"./checkpoints/\"\n",
    "MODEL_NAME = \"01_pytorch_workflow_model_0.pth\"\n",
    "# MODEL_NAME = str(d_model) + \"_\" + str(nhead) + \"_\" + str(num_layers) + \"_\" + MODEL_NAME\n",
    "MODEL_NAME =  \"transformer_encoder__lstm_\" + MODEL_NAME\n",
    "MODEL_SAVE_PATH = MODEL_PATH +\"/\"+ MODEL_NAME\n",
    "\n",
    "\n",
    "\n",
    "# 3. Save the model state dict \n",
    "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
    "\n",
    "torch.save(obj=model.state_dict(), # only saving the state_dict() only saves the models learned parameters\n",
    "           f=MODEL_SAVE_PATH) \n",
    "\n",
    "\n",
    "print(\"done saving!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_loop():\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_accuracy = []\n",
    "    for i, batch in enumerate(test_dataloader):\n",
    "        x, y, y_seq = batch\n",
    "        x = x.to(torch.float32)\n",
    "        y = y.to(torch.float32)\n",
    "        y_seq = y_seq.to(torch.float32)\n",
    "        \n",
    "        y_pred = model(x)\n",
    "        \n",
    "        total_inputs = 0\n",
    "        true_pred = []\n",
    "        \n",
    "        \n",
    "        for idx,y in enumerate(y_pred):\n",
    "            \n",
    "            total_inputs += 1\n",
    "            \n",
    "            output_argmax = torch.argmax(y)\n",
    "            gt_argmax = torch.argmax(y_seq[idx])\n",
    "        \n",
    "            if(output_argmax == gt_argmax):\n",
    "                true_pred.append(output_argmax)\n",
    "                \n",
    "            accuracy = len(true_pred)/total_inputs\n",
    "            \n",
    "            # print(\"Accuracy: \",accuracy)\n",
    "            total_accuracy.append(accuracy)\n",
    "            \n",
    "            \n",
    "                \n",
    "        loss = criterion(y_pred, y_seq)\n",
    "        # print(i, \"Loss: \", loss)\n",
    "        \n",
    "    print(\"Average accuracy: \", np.average(total_accuracy))\n",
    "    \n",
    "    \n",
    "evaluation_loop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
