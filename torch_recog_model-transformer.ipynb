{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y, seq_len):\n",
    "        self.X = torch.Tensor(X)\n",
    "        self.y = torch.LongTensor(y)\n",
    "        self.seq_len = seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_model, nhead, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout), num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(input_dim, d_model)\n",
    "        self.out = nn.Linear(d_model, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "  \n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Subject Trials:  32\n",
      "Test Subject Trials:  4\n",
      "(50127, 10)\n"
     ]
    }
   ],
   "source": [
    "def generate_data(subject_id, task, features, batch_size, seq_len):    \n",
    "    \n",
    "    csv_path = './ProcessedDatasets/' + task\n",
    "    \n",
    "    csv_files = glob.glob(csv_path + \"/*.csv\")\n",
    "    \n",
    "    train_df_list = []\n",
    "    test_df_list = []\n",
    "    \n",
    "    for file in csv_files:\n",
    "        if(subject_id in file):\n",
    "            test_df_list.append(pd.read_csv(file))\n",
    "#             print(file)\n",
    "        else:\n",
    "            train_df_list.append(pd.read_csv(file))\n",
    "            \n",
    "\n",
    "    print('Train Subject Trials: ',len(train_df_list))\n",
    "    print('Test Subject Trials: ',len(test_df_list))\n",
    "    \n",
    "    # Concatenate all DataFrames\n",
    "    train_df   = pd.concat(train_df_list, ignore_index=True)\n",
    "    test_df   = pd.concat(test_df_list, ignore_index=True)\n",
    "\n",
    "    \n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "\n",
    "    train_labels= train_df.pop('label')\n",
    "    train_features = train_df\n",
    "\n",
    "    test_labels= test_df.pop('label')\n",
    "    test_features = test_df\n",
    "\n",
    "\n",
    "    all_class_names = [\"G1\", 'G2', 'G3', 'G4', 'G5', 'G6', 'G8', 'G9', 'G10', 'G11', 'G12', 'G13', 'G14', 'G15']\n",
    "    lb.fit(all_class_names)\n",
    "\n",
    "    train_labels = lb.transform(train_labels)\n",
    "    test_labels = lb.transform(test_labels)\n",
    "    \n",
    "    train_x = train_features.to_numpy()\n",
    "    train_y = train_labels\n",
    "\n",
    "    test_x = test_features.to_numpy()\n",
    "    test_y = test_labels\n",
    "    \n",
    "    train_x = train_x[:,:features]\n",
    "    test_x = test_x[:,:features]\n",
    "    \n",
    "\n",
    "    train_dataset = TimeSeriesDataset(train_x, train_y, seq_len)\n",
    "    test_dataset = TimeSeriesDataset(test_x, test_y, seq_len)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "\n",
    "    \n",
    "generate_data(\"S02\",\"Knot_Tying\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training \n",
    "\n",
    "dataset = TimeSeriesDataset(X_train, y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=32)\n",
    "\n",
    "# best config yet: d_model = 64,  nhead=4, num_layers=2\n",
    "d_model = 64\n",
    "nhead=4\n",
    "num_layers=2\n",
    "\n",
    "model = TransformerModel(input_dim=X_train.shape[-1], output_dim=7, d_model=d_model, nhead=nhead, num_layers=num_layers)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# training loop\n",
    "for epoch in range(100):\n",
    "    running_loss = 0.0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        x, y = batch\n",
    "        x = x.to(torch.float32)\n",
    "        y_pred = model(x)\n",
    "\n",
    "        # print(y, y_pred)\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(dataloader):.6f}\")\n",
    "\n",
    "\n",
    "# 2. Create model save path \n",
    "MODEL_NAME = \"01_pytorch_workflow_model_0.pth\"\n",
    "MODEL_NAME = str(d_model) + \"_\" + str(nhead) + \"_\" + str(num_layers) + \"_\" + MODEL_NAME\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "\n",
    "\n",
    "# 3. Save the model state dict \n",
    "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
    "\n",
    "torch.save(obj=model.state_dict(), # only saving the state_dict() only saves the models learned parameters\n",
    "           f=MODEL_SAVE_PATH) \n",
    "\n",
    "\n",
    "print(\"done saving!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
