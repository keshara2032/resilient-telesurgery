{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, x_set, y_set, seq_len):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.seq_len)))\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = idx * self.seq_len\n",
    "        end_idx = (idx + 1) * self.seq_len\n",
    "\n",
    "        batch_x = self.x[start_idx:end_idx]\n",
    "        batch_y = self.y[start_idx:end_idx]\n",
    "\n",
    "        # Count the occurrences of each row\n",
    "        unique_rows, counts = np.unique(batch_y, axis=0, return_counts=True)\n",
    "\n",
    "        # Get the index of the row with the highest count\n",
    "        most_common_row_index = np.argmax(counts)\n",
    "\n",
    "        # Get the most common row\n",
    "        most_common_row = unique_rows[most_common_row_index]\n",
    "        \n",
    "        \n",
    "        batch_y_bin = most_common_row\n",
    "\n",
    "        # Convert NumPy arrays to PyTorch tensors\n",
    "        batch_x = torch.from_numpy(batch_x)\n",
    "        batch_y = torch.from_numpy(batch_y)\n",
    "        batch_y_bin = torch.from_numpy(batch_y_bin)\n",
    "\n",
    "        # Pad sequences to ensure they have the same length within the batch\n",
    "        pad_len = self.seq_len - batch_x.shape[0]\n",
    "        if pad_len > 0:\n",
    "            pad_shape = (pad_len,) + batch_x.shape[1:]\n",
    "            pad_shape_y = (pad_len,) + batch_y.shape[1:]\n",
    "\n",
    "            batch_x = torch.cat([batch_x, torch.zeros(pad_shape)], dim=0)\n",
    "            batch_y = torch.cat([batch_y, torch.zeros(pad_shape_y)], dim=0)\n",
    "\n",
    "        return batch_x, batch_y, batch_y_bin\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        indices = np.arange(len(self.x))\n",
    "        np.random.shuffle(indices)\n",
    "        self.x = self.x[indices]\n",
    "        self.y = self.y[indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8])\n"
     ]
    }
   ],
   "source": [
    "# target output size of 5\n",
    "\n",
    "class GlobalMaxPooling1D(nn.Module):\n",
    "\n",
    "    def __init__(self, data_format='channels_last'):\n",
    "        super(GlobalMaxPooling1D, self).__init__()\n",
    "        self.data_format = data_format\n",
    "        self.step_axis = 1 if self.data_format == 'channels_last' else 2\n",
    "\n",
    "    def forward(self, input):\n",
    "        return torch.max(input, axis=self.step_axis).values\n",
    "    \n",
    "\n",
    "m = GlobalMaxPooling1D()\n",
    "input = torch.randn(1, 64, 8)\n",
    "output = m(input)\n",
    "print(output.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, d_model, nhead, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout), num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        self.max_pool = GlobalMaxPooling1D()\n",
    "        self.fc = nn.Linear(input_dim, d_model)\n",
    "        self.out = nn.Linear(d_model, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        x = self.max_pool(x)\n",
    "  \n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Subject Trials:  32\n",
      "Test Subject Trials:  4\n",
      "0 torch.Size([30, 30, 10]) torch.Size([30, 30, 14]) torch.Size([30, 14])\n"
     ]
    }
   ],
   "source": [
    "def generate_data(subject_id, task, features, batch_size, seq_len):    \n",
    "    \n",
    "    csv_path = './ProcessedDatasets/' + task\n",
    "    # csv_path = './Dataset'\n",
    "    \n",
    "    csv_files = glob.glob(csv_path + \"/*.csv\")\n",
    "    \n",
    "    train_df_list = []\n",
    "    test_df_list = []\n",
    "    \n",
    "    for file in csv_files:\n",
    "        if(subject_id in file):\n",
    "            test_df_list.append(pd.read_csv(file))\n",
    "#             print(file)\n",
    "        else:\n",
    "            train_df_list.append(pd.read_csv(file))\n",
    "            \n",
    "\n",
    "    print('Train Subject Trials: ',len(train_df_list))\n",
    "    print('Test Subject Trials: ',len(test_df_list))\n",
    "    \n",
    "    # Concatenate all DataFrames\n",
    "    train_df   = pd.concat(train_df_list, ignore_index=True)\n",
    "    test_df   = pd.concat(test_df_list, ignore_index=True)\n",
    "\n",
    "    \n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "\n",
    "    train_labels= train_df.pop('label')\n",
    "    train_features = train_df\n",
    "\n",
    "    test_labels= test_df.pop('label')\n",
    "    test_features = test_df\n",
    "\n",
    "\n",
    "    all_class_names = [\"G1\", 'G2', 'G3', 'G4', 'G5', 'G6', 'G8', 'G9', 'G10', 'G11', 'G12', 'G13', 'G14', 'G15']\n",
    "    lb.fit(all_class_names)\n",
    "\n",
    "    train_labels = lb.transform(train_labels)\n",
    "    test_labels = lb.transform(test_labels)\n",
    "    \n",
    "    train_x = train_features.to_numpy()\n",
    "    train_y = train_labels\n",
    "\n",
    "    test_x = test_features.to_numpy()\n",
    "    test_y = test_labels\n",
    "    \n",
    "    train_x = train_x[:,:features]\n",
    "    test_x = test_x[:,:features]\n",
    "    \n",
    "\n",
    "    train_dataset = TimeSeriesDataset(train_x, train_y, seq_len)\n",
    "    test_dataset = TimeSeriesDataset(test_x, test_y, seq_len)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader, test_dataloader\n",
    " \n",
    "    \n",
    "    \n",
    "features = 10\n",
    "batch_size = 30\n",
    "seq_len = 30\n",
    "output_dim = 14\n",
    "\n",
    "train_dataloader, test_dataloader = generate_data(\"S02\",\"Knot_Tying\",features, batch_size, seq_len)\n",
    "\n",
    "for idx,batch in enumerate(train_dataloader):\n",
    "    \n",
    "    print(idx, batch[0].shape ,batch[1].shape ,batch[2].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu117\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def train_loop(dataloader,model,optimizer,criterion, epochs):\n",
    "    \n",
    "    # training loop\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            x, y, y_seq = batch\n",
    "            x = x.to(torch.float32)\n",
    "            y = y.to(torch.float32)\n",
    "            y_seq = y_seq.to(torch.float32)\n",
    "            \n",
    "            y_pred = model(x)\n",
    "\n",
    "            # print(y_pred.shape, y_seq.shape)\n",
    "            \n",
    "            loss = criterion(y_pred, y_seq)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(dataloader):.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_loop(dataloader, model,criterion):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_accuracy = []\n",
    "    for i, batch in enumerate(test_dataloader):\n",
    "        x, y, y_seq = batch\n",
    "        x = x.to(torch.float32)\n",
    "        y = y.to(torch.float32)\n",
    "        y_seq = y_seq.to(torch.float32)\n",
    "        \n",
    "        y_pred = model(x)\n",
    "        \n",
    "        total_inputs = 0\n",
    "        true_pred = []\n",
    "        \n",
    "        \n",
    "        for idx,y in enumerate(y_pred):\n",
    "            \n",
    "            total_inputs += 1\n",
    "            \n",
    "            output_argmax = torch.argmax(y)\n",
    "            gt_argmax = torch.argmax(y_seq[idx])\n",
    "        \n",
    "            if(output_argmax == gt_argmax):\n",
    "                true_pred.append(output_argmax)\n",
    "                \n",
    "            accuracy = len(true_pred)/total_inputs\n",
    "            \n",
    "            # print(\"Accuracy: \",accuracy)\n",
    "            total_accuracy.append(accuracy)\n",
    "            \n",
    "            \n",
    "                \n",
    "        loss = criterion(y_pred, y_seq)\n",
    "        # print(i, \"Loss: \", loss)\n",
    "        \n",
    "    avg_accuracy = np.average(total_accuracy)\n",
    "    print(\"Average accuracy: \", avg_accuracy)\n",
    "    return avg_accuracy\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Subject Trials:  32\n",
      "Test Subject Trials:  4\n",
      "Epoch 1, Loss: 1.673736\n",
      "Epoch 2, Loss: 1.424627\n",
      "Epoch 3, Loss: 1.292166\n",
      "Epoch 4, Loss: 1.177386\n",
      "Epoch 5, Loss: 1.086739\n",
      "Epoch 6, Loss: 1.003562\n",
      "Epoch 7, Loss: 0.947532\n",
      "Epoch 8, Loss: 0.882792\n",
      "Epoch 9, Loss: 0.835498\n",
      "Epoch 10, Loss: 0.801189\n",
      "Epoch 11, Loss: 0.751390\n",
      "Epoch 12, Loss: 0.722252\n",
      "Epoch 13, Loss: 0.682676\n",
      "Epoch 14, Loss: 0.645591\n",
      "Epoch 15, Loss: 0.612214\n",
      "Epoch 16, Loss: 0.589057\n",
      "Epoch 17, Loss: 0.555386\n",
      "Epoch 18, Loss: 0.530337\n",
      "Epoch 19, Loss: 0.501017\n",
      "Epoch 20, Loss: 0.480705\n",
      "Epoch 21, Loss: 0.468379\n",
      "Epoch 22, Loss: 0.465215\n",
      "Epoch 23, Loss: 0.432521\n",
      "Epoch 24, Loss: 0.399681\n",
      "Epoch 25, Loss: 0.379789\n",
      "Epoch 26, Loss: 0.361954\n",
      "Epoch 27, Loss: 0.343322\n",
      "Epoch 28, Loss: 0.342154\n",
      "Epoch 29, Loss: 0.339299\n",
      "Epoch 30, Loss: 0.310688\n",
      "Epoch 31, Loss: 0.290178\n",
      "Epoch 32, Loss: 0.274740\n",
      "Epoch 33, Loss: 0.268666\n",
      "Epoch 34, Loss: 0.258436\n",
      "Epoch 35, Loss: 0.233461\n",
      "Epoch 36, Loss: 0.251513\n",
      "Epoch 37, Loss: 0.244225\n",
      "Epoch 38, Loss: 0.230365\n",
      "Epoch 39, Loss: 0.221300\n",
      "Epoch 40, Loss: 0.218335\n",
      "Epoch 41, Loss: 0.215247\n",
      "Epoch 42, Loss: 0.197936\n",
      "Epoch 43, Loss: 0.190517\n",
      "Epoch 44, Loss: 0.204434\n",
      "Epoch 45, Loss: 0.243764\n",
      "Epoch 46, Loss: 0.210517\n",
      "Epoch 47, Loss: 0.178506\n",
      "Epoch 48, Loss: 0.157084\n",
      "Epoch 49, Loss: 0.143536\n",
      "Epoch 50, Loss: 0.149453\n",
      "Epoch 51, Loss: 0.146931\n",
      "Epoch 52, Loss: 0.150509\n",
      "Epoch 53, Loss: 0.173023\n",
      "Epoch 54, Loss: 0.148161\n",
      "Epoch 55, Loss: 0.135405\n",
      "Epoch 56, Loss: 0.143064\n",
      "Epoch 57, Loss: 0.144579\n",
      "Epoch 58, Loss: 0.156973\n",
      "Epoch 59, Loss: 0.126658\n",
      "Epoch 60, Loss: 0.139581\n",
      "Epoch 61, Loss: 0.142720\n",
      "Epoch 62, Loss: 0.118652\n",
      "Epoch 63, Loss: 0.130161\n",
      "Epoch 64, Loss: 0.111320\n",
      "Epoch 65, Loss: 0.112558\n",
      "Epoch 66, Loss: 0.109454\n",
      "Epoch 67, Loss: 0.118580\n",
      "Epoch 68, Loss: 0.106324\n",
      "Epoch 69, Loss: 0.099001\n",
      "Epoch 70, Loss: 0.098561\n",
      "Epoch 71, Loss: 0.131973\n",
      "Epoch 72, Loss: 0.158967\n",
      "Epoch 73, Loss: 0.129424\n",
      "Epoch 74, Loss: 0.127420\n",
      "Epoch 75, Loss: 0.092895\n",
      "Epoch 76, Loss: 0.078263\n",
      "Epoch 77, Loss: 0.089840\n",
      "Epoch 78, Loss: 0.083660\n",
      "Epoch 79, Loss: 0.084130\n",
      "Epoch 80, Loss: 0.090712\n",
      "Epoch 81, Loss: 0.086719\n",
      "Epoch 82, Loss: 0.082475\n",
      "Epoch 83, Loss: 0.072633\n",
      "Epoch 84, Loss: 0.078700\n",
      "Epoch 85, Loss: 0.094959\n",
      "Epoch 86, Loss: 0.089957\n",
      "Epoch 87, Loss: 0.086635\n",
      "Epoch 88, Loss: 0.083530\n",
      "Epoch 89, Loss: 0.064588\n",
      "Epoch 90, Loss: 0.072828\n",
      "Epoch 91, Loss: 0.065425\n",
      "Epoch 92, Loss: 0.069890\n",
      "Epoch 93, Loss: 0.068457\n",
      "Epoch 94, Loss: 0.064007\n",
      "Epoch 95, Loss: 0.063584\n",
      "Epoch 96, Loss: 0.072999\n",
      "Epoch 97, Loss: 0.076377\n",
      "Epoch 98, Loss: 0.080911\n",
      "Epoch 99, Loss: 0.109546\n",
      "Epoch 100, Loss: 0.073733\n",
      "Average accuracy:  0.43284258356082583\n",
      "Train Subject Trials:  31\n",
      "Test Subject Trials:  5\n",
      "Epoch 1, Loss: 0.753001\n",
      "Epoch 2, Loss: 0.369152\n",
      "Epoch 3, Loss: 0.233738\n",
      "Epoch 4, Loss: 0.177485\n",
      "Epoch 5, Loss: 0.141139\n",
      "Epoch 6, Loss: 0.114336\n",
      "Epoch 7, Loss: 0.095349\n",
      "Epoch 8, Loss: 0.080546\n",
      "Epoch 9, Loss: 0.072796\n",
      "Epoch 10, Loss: 0.073605\n",
      "Epoch 11, Loss: 0.109035\n",
      "Epoch 12, Loss: 0.109325\n",
      "Epoch 13, Loss: 0.113916\n",
      "Epoch 14, Loss: 0.071244\n",
      "Epoch 15, Loss: 0.061251\n",
      "Epoch 16, Loss: 0.046689\n",
      "Epoch 17, Loss: 0.034537\n",
      "Epoch 18, Loss: 0.034691\n",
      "Epoch 19, Loss: 0.043257\n",
      "Epoch 20, Loss: 0.062635\n",
      "Epoch 21, Loss: 0.064539\n",
      "Epoch 22, Loss: 0.056693\n",
      "Epoch 23, Loss: 0.089466\n",
      "Epoch 24, Loss: 0.040425\n",
      "Epoch 25, Loss: 0.046777\n",
      "Epoch 26, Loss: 0.045596\n",
      "Epoch 27, Loss: 0.033303\n",
      "Epoch 28, Loss: 0.028784\n",
      "Epoch 29, Loss: 0.036688\n",
      "Epoch 30, Loss: 0.027675\n",
      "Epoch 31, Loss: 0.027542\n",
      "Epoch 32, Loss: 0.060495\n",
      "Epoch 33, Loss: 0.097403\n",
      "Epoch 34, Loss: 0.066587\n",
      "Epoch 35, Loss: 0.052571\n",
      "Epoch 36, Loss: 0.035491\n",
      "Epoch 37, Loss: 0.029559\n",
      "Epoch 38, Loss: 0.029464\n",
      "Epoch 39, Loss: 0.023700\n",
      "Epoch 40, Loss: 0.026548\n",
      "Epoch 41, Loss: 0.023268\n",
      "Epoch 42, Loss: 0.030329\n",
      "Epoch 43, Loss: 0.037969\n",
      "Epoch 44, Loss: 0.041897\n",
      "Epoch 45, Loss: 0.033944\n",
      "Epoch 46, Loss: 0.028669\n",
      "Epoch 47, Loss: 0.031362\n",
      "Epoch 48, Loss: 0.024438\n",
      "Epoch 49, Loss: 0.018885\n",
      "Epoch 50, Loss: 0.019780\n",
      "Epoch 51, Loss: 0.023880\n",
      "Epoch 52, Loss: 0.036054\n",
      "Epoch 53, Loss: 0.083087\n",
      "Epoch 54, Loss: 0.080863\n",
      "Epoch 55, Loss: 0.053356\n",
      "Epoch 56, Loss: 0.025258\n",
      "Epoch 57, Loss: 0.016479\n",
      "Epoch 58, Loss: 0.010908\n",
      "Epoch 59, Loss: 0.010368\n",
      "Epoch 60, Loss: 0.012594\n",
      "Epoch 61, Loss: 0.017651\n",
      "Epoch 62, Loss: 0.018114\n",
      "Epoch 63, Loss: 0.019283\n",
      "Epoch 64, Loss: 0.029016\n",
      "Epoch 65, Loss: 0.026590\n",
      "Epoch 66, Loss: 0.025686\n",
      "Epoch 67, Loss: 0.084437\n",
      "Epoch 68, Loss: 0.129468\n",
      "Epoch 69, Loss: 0.078708\n",
      "Epoch 70, Loss: 0.025106\n",
      "Epoch 71, Loss: 0.011609\n",
      "Epoch 72, Loss: 0.007499\n",
      "Epoch 73, Loss: 0.005876\n",
      "Epoch 74, Loss: 0.005242\n",
      "Epoch 75, Loss: 0.005113\n",
      "Epoch 76, Loss: 0.005967\n",
      "Epoch 77, Loss: 0.007373\n",
      "Epoch 78, Loss: 0.016634\n",
      "Epoch 79, Loss: 0.036429\n",
      "Epoch 80, Loss: 0.036868\n",
      "Epoch 81, Loss: 0.035357\n",
      "Epoch 82, Loss: 0.019982\n",
      "Epoch 83, Loss: 0.017182\n",
      "Epoch 84, Loss: 0.012201\n",
      "Epoch 85, Loss: 0.017024\n",
      "Epoch 86, Loss: 0.021643\n",
      "Epoch 87, Loss: 0.026129\n",
      "Epoch 88, Loss: 0.120755\n",
      "Epoch 89, Loss: 0.219607\n",
      "Epoch 90, Loss: 0.040858\n",
      "Epoch 91, Loss: 0.010595\n",
      "Epoch 92, Loss: 0.007357\n",
      "Epoch 93, Loss: 0.006097\n",
      "Epoch 94, Loss: 0.005401\n",
      "Epoch 95, Loss: 0.004810\n",
      "Epoch 96, Loss: 0.004397\n",
      "Epoch 97, Loss: 0.004065\n",
      "Epoch 98, Loss: 0.004125\n",
      "Epoch 99, Loss: 0.005179\n",
      "Epoch 100, Loss: 0.009815\n",
      "Average accuracy:  0.8164599059830342\n",
      "Train Subject Trials:  31\n",
      "Test Subject Trials:  5\n",
      "Epoch 1, Loss: 0.800530\n",
      "Epoch 2, Loss: 0.318606\n",
      "Epoch 3, Loss: 0.163114\n",
      "Epoch 4, Loss: 0.107453\n",
      "Epoch 5, Loss: 0.084190\n",
      "Epoch 6, Loss: 0.069573\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mfor\u001b[39;00m subject \u001b[39min\u001b[39;00m subjects:\n\u001b[1;32m     23\u001b[0m     train_dataloader, test_dataloader \u001b[39m=\u001b[39m generate_data(subject,task,features, batch_size, seq_len)\n\u001b[0;32m---> 25\u001b[0m     train_loop(dataloader\u001b[39m=\u001b[39;49mtrain_dataloader, model\u001b[39m=\u001b[39;49mmodel, optimizer\u001b[39m=\u001b[39;49moptimizer, criterion\u001b[39m=\u001b[39;49mcriterion, epochs\u001b[39m=\u001b[39;49mepochs)\n\u001b[1;32m     27\u001b[0m     evaluation_loop(dataloader\u001b[39m=\u001b[39mtest_dataloader, model\u001b[39m=\u001b[39mmodel, criterion\u001b[39m=\u001b[39mcriterion)\n",
      "Cell \u001b[0;32mIn[7], line 19\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, optimizer, criterion, epochs)\u001b[0m\n\u001b[1;32m     17\u001b[0m     loss \u001b[39m=\u001b[39m criterion(y_pred, y_seq)\n\u001b[1;32m     18\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 19\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     20\u001b[0m     running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     22\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mrunning_loss\u001b[39m \u001b[39m\u001b[39m/\u001b[39m\u001b[39m \u001b[39m\u001b[39mlen\u001b[39m(dataloader)\u001b[39m:\u001b[39;00m\u001b[39m.6f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    130\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[1;32m    133\u001b[0m         group,\n\u001b[1;32m    134\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    139\u001b[0m         state_steps)\n\u001b[0;32m--> 141\u001b[0m     adam(\n\u001b[1;32m    142\u001b[0m         params_with_grad,\n\u001b[1;32m    143\u001b[0m         grads,\n\u001b[1;32m    144\u001b[0m         exp_avgs,\n\u001b[1;32m    145\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    146\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    147\u001b[0m         state_steps,\n\u001b[1;32m    148\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    149\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    150\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    151\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    152\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    153\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    154\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    155\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    156\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    157\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    158\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    159\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    160\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/optim/adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 281\u001b[0m func(params,\n\u001b[1;32m    282\u001b[0m      grads,\n\u001b[1;32m    283\u001b[0m      exp_avgs,\n\u001b[1;32m    284\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    285\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    286\u001b[0m      state_steps,\n\u001b[1;32m    287\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    288\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    289\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    290\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    291\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    292\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    293\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    294\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    295\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    296\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    297\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/optim/adam.py:332\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[39massert\u001b[39;00m param\u001b[39m.\u001b[39mis_cuda \u001b[39mand\u001b[39;00m step_t\u001b[39m.\u001b[39mis_cuda, \u001b[39m\"\u001b[39m\u001b[39mIf capturable=True, params and state_steps must be CUDA tensors.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    331\u001b[0m \u001b[39m# update step\u001b[39;00m\n\u001b[0;32m--> 332\u001b[0m step_t \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    334\u001b[0m \u001b[39mif\u001b[39;00m weight_decay \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    335\u001b[0m     grad \u001b[39m=\u001b[39m grad\u001b[39m.\u001b[39madd(param, alpha\u001b[39m=\u001b[39mweight_decay)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "features = 22\n",
    "batch_size = 30\n",
    "seq_len = 10\n",
    "output_dim = 14\n",
    "\n",
    "task = \"Knot_Tying\"\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "# best config yet: d_model = 64,  nhead=4, num_layers=2\n",
    "d_model = 64\n",
    "nhead=4\n",
    "num_layers=2\n",
    "\n",
    "model = TransformerModel(input_dim=features, output_dim=output_dim, d_model=d_model, nhead=nhead, num_layers=num_layers)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "subjects = ['S02','S03','S04','S05','S06','S07','S08','S09']\n",
    "\n",
    "accuracy = []\n",
    "for subject in subjects:\n",
    "    \n",
    "    train_dataloader, test_dataloader = generate_data(subject,task,features, batch_size, seq_len)\n",
    "\n",
    "    train_loop(dataloader=train_dataloader, model=model, optimizer=optimizer, criterion=criterion, epochs=epochs)\n",
    "    \n",
    "    acc = evaluation_loop(dataloader=test_dataloader, model=model, criterion=criterion)\n",
    "    \n",
    "    accuracy.append({'subject':subject, 'accuracy':acc})\n",
    "\n",
    "print(accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: ./checkpoints//64_4_2_01_pytorch_workflow_model_0.pth\n",
      "done saving!\n"
     ]
    }
   ],
   "source": [
    "# 2. Create model save path \n",
    "MODEL_PATH = \"./checkpoints/\"\n",
    "MODEL_NAME = \"01_pytorch_workflow_model_0.pth\"\n",
    "MODEL_NAME = str(d_model) + \"_\" + str(nhead) + \"_\" + str(num_layers) + \"_\" + MODEL_NAME\n",
    "MODEL_SAVE_PATH = MODEL_PATH +\"/\"+ MODEL_NAME\n",
    "\n",
    "\n",
    "\n",
    "# 3. Save the model state dict \n",
    "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
    "\n",
    "torch.save(obj=model.state_dict(), # only saving the state_dict() only saves the models learned parameters\n",
    "           f=MODEL_SAVE_PATH) \n",
    "\n",
    "\n",
    "print(\"done saving!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy:  0.28562257379625866\n"
     ]
    }
   ],
   "source": [
    "def evaluation_loop():\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_accuracy = []\n",
    "    for i, batch in enumerate(test_dataloader):\n",
    "        x, y, y_seq = batch\n",
    "        x = x.to(torch.float32)\n",
    "        y = y.to(torch.float32)\n",
    "        y_seq = y_seq.to(torch.float32)\n",
    "        \n",
    "        y_pred = model(x)\n",
    "        \n",
    "        total_inputs = 0\n",
    "        true_pred = []\n",
    "        \n",
    "        \n",
    "        for idx,y in enumerate(y_pred):\n",
    "            \n",
    "            total_inputs += 1\n",
    "            \n",
    "            output_argmax = torch.argmax(y)\n",
    "            gt_argmax = torch.argmax(y_seq[idx])\n",
    "        \n",
    "            if(output_argmax == gt_argmax):\n",
    "                true_pred.append(output_argmax)\n",
    "                \n",
    "            accuracy = len(true_pred)/total_inputs\n",
    "            \n",
    "            # print(\"Accuracy: \",accuracy)\n",
    "            total_accuracy.append(accuracy)\n",
    "            \n",
    "            \n",
    "                \n",
    "        loss = criterion(y_pred, y_seq)\n",
    "        # print(i, \"Loss: \", loss)\n",
    "        \n",
    "    print(\"Average accuracy: \", np.average(total_accuracy))\n",
    "    \n",
    "    \n",
    "evaluation_loop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
