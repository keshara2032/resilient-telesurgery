{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (3795282794.py, line 44)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [174]\u001b[0;36m\u001b[0m\n\u001b[0;31m    def __init__(self, input_dim, d_model, heads, num_layers,dropout, max_len, output_classes):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, heads, num_layers,dropout, max_len, output_classes):\n",
    "        super().__init__()\n",
    "        self.encoder_layer= nn.TransformerEncoderLayer(d_model=d_model, nhead=heads)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        self.positional_encoder = PositionalEncoding(d_model=d_model, dropout=dropout, max_len=max_len)\n",
    "        self.linear = nn.Linear(in_features=d_model, out_features=output_classes)\n",
    "        self.fc = nn.Linear(input_dim, d_model)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.fc(x)\n",
    "        x1 = self.positional_encoder(x1)\n",
    "        x2 = self.encoder(x1)\n",
    "        out = self.linear(x2)\n",
    "        # out = self.softmax(out)\n",
    "        return out\n",
    "        \n",
    "\n",
    "\n",
    "kinematic_features = 22 #input_dim\n",
    "dmodel = 64 #dmodel\n",
    "heads = 4\n",
    "num_layers = 2\n",
    "dropout = 0.1\n",
    "max_len = 100\n",
    "output_classes = 7\n",
    "\n",
    "# src = torch.rand(10, 10, kinematic_features)\n",
    "\n",
    "model = Transformer(kinematic_features,dmodel, heads,num_layers,dropout,max_len,output_classes)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "gesture_labels_directory = './new_dataset/peg_transfer/peg_transfer/labeled/gestures/'\n",
    "window_size = 10 # 10 frame windows (kinematic data)\n",
    "enite_dataset = []\n",
    "counter = 0\n",
    "\n",
    "\n",
    "\n",
    "class KinematicDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,data_directory, window_size):\n",
    "        \n",
    "        self.directory = data_directory\n",
    "        self.window_size = window_size\n",
    "        self.le = preprocessing.LabelEncoder()\n",
    "        self.enc = preprocessing.OneHotEncoder(sparse_output=False)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(self.device)\n",
    "        (self.X, self.Y) = self._load_data()\n",
    "        self.X = self.X.to(self.device)\n",
    "        self.Y = self.Y.to(self.device)\n",
    "        \n",
    "        \n",
    "    def _load_data(self):\n",
    "        x = []\n",
    "        y = []\n",
    "        self.windows_per_file = []\n",
    "        \n",
    "        for filename in os.listdir(self.directory):\n",
    "            f = os.path.join(self.directory, filename)\n",
    "            # checking if it is a file\n",
    "            if os.path.isfile(f):\n",
    "                kinematics_data = pd.read_csv(f)\n",
    "                kin_data = kinematics_data.iloc[:,:-1]\n",
    "                kin_label = kinematics_data.iloc[:,-1]\n",
    "                \n",
    "                x.append(kin_data.values)\n",
    "                y.append(kin_label.values)\n",
    "                \n",
    "                \n",
    "                \n",
    "                if len(self.windows_per_file) == 0:\n",
    "                    self.windows_per_file = [len(kin_label) - self.window_size + 1]\n",
    "                else:\n",
    "                    self.windows_per_file.append(self.windows_per_file[-1] + len(kin_label) - self.window_size + 1)\n",
    "        \n",
    "        self.le.fit(y[0])\n",
    "        y = [self.le.transform(yi) for yi in y]\n",
    "        \n",
    "        y = [yi.reshape(len(yi), 1) for yi in y]\n",
    "        \n",
    "        y = [self.enc.fit_transform(yi) for yi in y]\n",
    "        \n",
    "        x = np.concatenate(x)\n",
    "        y = np.concatenate(y)\n",
    "        \n",
    "        x = torch.from_numpy(x)\n",
    "        y = torch.from_numpy(y)\n",
    "        \n",
    "        x = x.to(torch.float32)\n",
    "        return (x, y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        # this should return the size of the dataset\n",
    "        return self.Y.shape[0]-window_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # this should return one sample from the dataset\n",
    "        features = self.X[idx : idx + window_size]\n",
    "        target = self.Y[idx : idx + window_size]\n",
    "        return features, target\n",
    "                    \n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cold' 'cold' 'warm' 'cold' 'hot' 'hot' 'warm' 'cold' 'warm' 'hot']\n",
      "[0 0 2 0 1 1 2 0 2 1]\n",
      "[[0]\n",
      " [0]\n",
      " [2]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [0]\n",
      " [2]\n",
      " [1]]\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]]\n",
      "['cold']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kesharaw/anaconda3/envs/rsurgery/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# define example\n",
    "data = ['cold', 'cold', 'warm', 'cold', 'hot', 'hot', 'warm', 'cold', 'warm', 'hot']\n",
    "values = array(data)\n",
    "print(values)\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "print(integer_encoded)\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "\n",
    "print(integer_encoded)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)\n",
    "# invert first example\n",
    "inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n",
    "print(inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "119279 29820\n",
      "torch.Size([149109, 22]) torch.Size([149109, 22])\n",
      "torch.Size([149109, 7]) torch.Size([149109, 7])\n"
     ]
    }
   ],
   "source": [
    "# train_data, test_data = \n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "dataset = KinematicDataset(gesture_labels_directory,10)\n",
    "\n",
    "# use 20% of training data for validation\n",
    "train_set_size = int(len(dataset) * 0.8)\n",
    "valid_set_size = len(dataset) - train_set_size\n",
    "print(train_set_size, valid_set_size)\n",
    "\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [train_set_size, valid_set_size],generator=seed)\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_set, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)\n",
    "\n",
    "print(train_set.dataset.X.shape, test_set.dataset.X.shape)\n",
    "print(train_set.dataset.Y.shape, test_set.dataset.Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] loss: 0.023\n",
      "[1,   100] loss: 0.016\n",
      "[1,   150] loss: 0.013\n",
      "[1,   200] loss: 0.011\n",
      "[1,   250] loss: 0.010\n",
      "[1,   300] loss: 0.010\n",
      "[1,   350] loss: 0.009\n",
      "[1,   400] loss: 0.009\n",
      "[1,   450] loss: 0.008\n",
      "[1,   500] loss: 0.009\n",
      "[1,   550] loss: 0.008\n",
      "[1,   600] loss: 0.008\n",
      "[1,   650] loss: 0.008\n",
      "[1,   700] loss: 0.007\n",
      "[1,   750] loss: 0.008\n",
      "[1,   800] loss: 0.008\n",
      "[1,   850] loss: 0.007\n",
      "[1,   900] loss: 0.007\n",
      "[1,   950] loss: 0.007\n",
      "[1,  1000] loss: 0.007\n",
      "[1,  1050] loss: 0.007\n",
      "[1,  1100] loss: 0.007\n",
      "[1,  1150] loss: 0.007\n",
      "[1,  1200] loss: 0.007\n",
      "[1,  1250] loss: 0.007\n",
      "[1,  1300] loss: 0.007\n",
      "[1,  1350] loss: 0.007\n",
      "[1,  1400] loss: 0.007\n",
      "[1,  1450] loss: 0.007\n",
      "[1,  1500] loss: 0.006\n",
      "[1,  1550] loss: 0.007\n",
      "[1,  1600] loss: 0.007\n",
      "[1,  1650] loss: 0.007\n",
      "[1,  1700] loss: 0.006\n",
      "[1,  1750] loss: 0.006\n",
      "[1,  1800] loss: 0.006\n",
      "[1,  1850] loss: 0.006\n",
      "[1,  1900] loss: 0.007\n",
      "[1,  1950] loss: 0.006\n",
      "[1,  2000] loss: 0.006\n",
      "[1,  2050] loss: 0.007\n",
      "[1,  2100] loss: 0.006\n",
      "[1,  2150] loss: 0.006\n",
      "[1,  2200] loss: 0.006\n",
      "[1,  2250] loss: 0.006\n",
      "[1,  2300] loss: 0.007\n",
      "[1,  2350] loss: 0.006\n",
      "[1,  2400] loss: 0.006\n",
      "[1,  2450] loss: 0.006\n",
      "[1,  2500] loss: 0.006\n",
      "[1,  2550] loss: 0.006\n",
      "[1,  2600] loss: 0.006\n",
      "[1,  2650] loss: 0.006\n",
      "[1,  2700] loss: 0.006\n",
      "[1,  2750] loss: 0.006\n",
      "[1,  2800] loss: 0.006\n",
      "[1,  2850] loss: 0.006\n",
      "[1,  2900] loss: 0.006\n",
      "[1,  2950] loss: 0.006\n",
      "[1,  3000] loss: 0.006\n",
      "[1,  3050] loss: 0.006\n",
      "[1,  3100] loss: 0.006\n",
      "[1,  3150] loss: 0.006\n",
      "[1,  3200] loss: 0.006\n",
      "[1,  3250] loss: 0.006\n",
      "[1,  3300] loss: 0.006\n",
      "[1,  3350] loss: 0.006\n",
      "[1,  3400] loss: 0.006\n",
      "[1,  3450] loss: 0.006\n",
      "[1,  3500] loss: 0.007\n",
      "[1,  3550] loss: 0.005\n",
      "[1,  3600] loss: 0.006\n",
      "[1,  3650] loss: 0.006\n",
      "[1,  3700] loss: 0.006\n",
      "[2,    50] loss: 0.006\n",
      "[2,   100] loss: 0.006\n",
      "[2,   150] loss: 0.006\n",
      "[2,   200] loss: 0.006\n",
      "[2,   250] loss: 0.006\n",
      "[2,   300] loss: 0.006\n",
      "[2,   350] loss: 0.006\n",
      "[2,   400] loss: 0.006\n",
      "[2,   450] loss: 0.005\n",
      "[2,   500] loss: 0.006\n",
      "[2,   550] loss: 0.006\n",
      "[2,   600] loss: 0.006\n",
      "[2,   650] loss: 0.006\n",
      "[2,   700] loss: 0.006\n",
      "[2,   750] loss: 0.006\n",
      "[2,   800] loss: 0.006\n",
      "[2,   850] loss: 0.006\n",
      "[2,   900] loss: 0.006\n",
      "[2,   950] loss: 0.006\n",
      "[2,  1000] loss: 0.006\n",
      "[2,  1050] loss: 0.006\n",
      "[2,  1100] loss: 0.005\n",
      "[2,  1150] loss: 0.006\n",
      "[2,  1200] loss: 0.006\n",
      "[2,  1250] loss: 0.006\n",
      "[2,  1300] loss: 0.006\n",
      "[2,  1350] loss: 0.006\n",
      "[2,  1400] loss: 0.006\n",
      "[2,  1450] loss: 0.006\n",
      "[2,  1500] loss: 0.005\n",
      "[2,  1550] loss: 0.006\n",
      "[2,  1600] loss: 0.006\n",
      "[2,  1650] loss: 0.006\n",
      "[2,  1700] loss: 0.006\n",
      "[2,  1750] loss: 0.005\n",
      "[2,  1800] loss: 0.005\n",
      "[2,  1850] loss: 0.006\n",
      "[2,  1900] loss: 0.006\n",
      "[2,  1950] loss: 0.006\n",
      "[2,  2000] loss: 0.006\n",
      "[2,  2050] loss: 0.006\n",
      "[2,  2100] loss: 0.006\n",
      "[2,  2150] loss: 0.005\n",
      "[2,  2200] loss: 0.006\n",
      "[2,  2250] loss: 0.006\n",
      "[2,  2300] loss: 0.006\n",
      "[2,  2350] loss: 0.006\n",
      "[2,  2400] loss: 0.006\n",
      "[2,  2450] loss: 0.006\n",
      "[2,  2500] loss: 0.006\n",
      "[2,  2550] loss: 0.006\n",
      "[2,  2600] loss: 0.006\n",
      "[2,  2650] loss: 0.006\n",
      "[2,  2700] loss: 0.006\n",
      "[2,  2750] loss: 0.006\n",
      "[2,  2800] loss: 0.006\n",
      "[2,  2850] loss: 0.006\n",
      "[2,  2900] loss: 0.005\n",
      "[2,  2950] loss: 0.005\n",
      "[2,  3000] loss: 0.006\n",
      "[2,  3050] loss: 0.006\n",
      "[2,  3100] loss: 0.006\n",
      "[2,  3150] loss: 0.006\n",
      "[2,  3200] loss: 0.006\n",
      "[2,  3250] loss: 0.005\n",
      "[2,  3300] loss: 0.005\n",
      "[2,  3350] loss: 0.005\n",
      "[2,  3400] loss: 0.005\n",
      "[2,  3450] loss: 0.006\n",
      "[2,  3500] loss: 0.006\n",
      "[2,  3550] loss: 0.005\n",
      "[2,  3600] loss: 0.006\n",
      "[2,  3650] loss: 0.006\n",
      "[2,  3700] loss: 0.005\n",
      "[3,    50] loss: 0.005\n",
      "[3,   100] loss: 0.006\n",
      "[3,   150] loss: 0.005\n",
      "[3,   200] loss: 0.006\n",
      "[3,   250] loss: 0.005\n",
      "[3,   300] loss: 0.005\n",
      "[3,   350] loss: 0.005\n",
      "[3,   400] loss: 0.006\n",
      "[3,   450] loss: 0.005\n",
      "[3,   500] loss: 0.006\n",
      "[3,   550] loss: 0.006\n",
      "[3,   600] loss: 0.006\n",
      "[3,   650] loss: 0.005\n",
      "[3,   700] loss: 0.005\n",
      "[3,   750] loss: 0.006\n",
      "[3,   800] loss: 0.005\n",
      "[3,   850] loss: 0.005\n",
      "[3,   900] loss: 0.005\n",
      "[3,   950] loss: 0.006\n",
      "[3,  1000] loss: 0.005\n",
      "[3,  1050] loss: 0.005\n",
      "[3,  1100] loss: 0.005\n",
      "[3,  1150] loss: 0.005\n",
      "[3,  1200] loss: 0.006\n",
      "[3,  1250] loss: 0.005\n",
      "[3,  1300] loss: 0.006\n",
      "[3,  1350] loss: 0.005\n",
      "[3,  1400] loss: 0.005\n",
      "[3,  1450] loss: 0.006\n",
      "[3,  1500] loss: 0.005\n",
      "[3,  1550] loss: 0.005\n",
      "[3,  1600] loss: 0.006\n",
      "[3,  1650] loss: 0.005\n",
      "[3,  1700] loss: 0.005\n",
      "[3,  1750] loss: 0.005\n",
      "[3,  1800] loss: 0.005\n",
      "[3,  1850] loss: 0.005\n",
      "[3,  1900] loss: 0.006\n",
      "[3,  1950] loss: 0.005\n",
      "[3,  2000] loss: 0.005\n",
      "[3,  2050] loss: 0.005\n",
      "[3,  2100] loss: 0.006\n",
      "[3,  2150] loss: 0.005\n",
      "[3,  2200] loss: 0.005\n",
      "[3,  2250] loss: 0.006\n",
      "[3,  2300] loss: 0.006\n",
      "[3,  2350] loss: 0.005\n",
      "[3,  2400] loss: 0.005\n",
      "[3,  2450] loss: 0.005\n",
      "[3,  2500] loss: 0.005\n",
      "[3,  2550] loss: 0.005\n",
      "[3,  2600] loss: 0.005\n",
      "[3,  2650] loss: 0.005\n",
      "[3,  2700] loss: 0.005\n",
      "[3,  2750] loss: 0.005\n",
      "[3,  2800] loss: 0.005\n",
      "[3,  2850] loss: 0.006\n",
      "[3,  2900] loss: 0.005\n",
      "[3,  2950] loss: 0.005\n",
      "[3,  3000] loss: 0.006\n",
      "[3,  3050] loss: 0.006\n",
      "[3,  3100] loss: 0.006\n",
      "[3,  3150] loss: 0.005\n",
      "[3,  3200] loss: 0.006\n",
      "[3,  3250] loss: 0.005\n",
      "[3,  3300] loss: 0.005\n",
      "[3,  3350] loss: 0.005\n",
      "[3,  3400] loss: 0.005\n",
      "[3,  3450] loss: 0.005\n",
      "[3,  3500] loss: 0.006\n",
      "[3,  3550] loss: 0.005\n",
      "[3,  3600] loss: 0.006\n",
      "[3,  3650] loss: 0.006\n",
      "[3,  3700] loss: 0.005\n",
      "[4,    50] loss: 0.005\n",
      "[4,   100] loss: 0.005\n",
      "[4,   150] loss: 0.005\n",
      "[4,   200] loss: 0.005\n",
      "[4,   250] loss: 0.005\n",
      "[4,   300] loss: 0.005\n",
      "[4,   350] loss: 0.005\n",
      "[4,   400] loss: 0.006\n",
      "[4,   450] loss: 0.005\n",
      "[4,   500] loss: 0.005\n",
      "[4,   550] loss: 0.006\n",
      "[4,   600] loss: 0.006\n",
      "[4,   650] loss: 0.005\n",
      "[4,   700] loss: 0.005\n",
      "[4,   750] loss: 0.005\n",
      "[4,   800] loss: 0.005\n",
      "[4,   850] loss: 0.005\n",
      "[4,   900] loss: 0.005\n",
      "[4,   950] loss: 0.005\n",
      "[4,  1000] loss: 0.005\n",
      "[4,  1050] loss: 0.005\n",
      "[4,  1100] loss: 0.005\n",
      "[4,  1150] loss: 0.005\n",
      "[4,  1200] loss: 0.005\n",
      "[4,  1250] loss: 0.005\n",
      "[4,  1300] loss: 0.005\n",
      "[4,  1350] loss: 0.005\n",
      "[4,  1400] loss: 0.005\n",
      "[4,  1450] loss: 0.005\n",
      "[4,  1500] loss: 0.005\n",
      "[4,  1550] loss: 0.005\n",
      "[4,  1600] loss: 0.005\n",
      "[4,  1650] loss: 0.006\n",
      "[4,  1700] loss: 0.005\n",
      "[4,  1750] loss: 0.005\n",
      "[4,  1800] loss: 0.005\n",
      "[4,  1850] loss: 0.005\n",
      "[4,  1900] loss: 0.005\n",
      "[4,  1950] loss: 0.005\n",
      "[4,  2000] loss: 0.005\n",
      "[4,  2050] loss: 0.005\n",
      "[4,  2100] loss: 0.005\n",
      "[4,  2150] loss: 0.005\n",
      "[4,  2200] loss: 0.005\n",
      "[4,  2250] loss: 0.005\n",
      "[4,  2300] loss: 0.005\n",
      "[4,  2350] loss: 0.005\n",
      "[4,  2400] loss: 0.005\n",
      "[4,  2450] loss: 0.005\n",
      "[4,  2500] loss: 0.005\n",
      "[4,  2550] loss: 0.005\n",
      "[4,  2600] loss: 0.005\n",
      "[4,  2650] loss: 0.005\n",
      "[4,  2700] loss: 0.005\n",
      "[4,  2750] loss: 0.005\n",
      "[4,  2800] loss: 0.005\n",
      "[4,  2850] loss: 0.005\n",
      "[4,  2900] loss: 0.005\n",
      "[4,  2950] loss: 0.005\n",
      "[4,  3000] loss: 0.006\n",
      "[4,  3050] loss: 0.006\n",
      "[4,  3100] loss: 0.005\n",
      "[4,  3150] loss: 0.005\n",
      "[4,  3200] loss: 0.005\n",
      "[4,  3250] loss: 0.005\n",
      "[4,  3300] loss: 0.005\n",
      "[4,  3350] loss: 0.005\n",
      "[4,  3400] loss: 0.005\n",
      "[4,  3450] loss: 0.005\n",
      "[4,  3500] loss: 0.006\n",
      "[4,  3550] loss: 0.004\n",
      "[4,  3600] loss: 0.006\n",
      "[4,  3650] loss: 0.006\n",
      "[4,  3700] loss: 0.005\n",
      "[5,    50] loss: 0.005\n",
      "[5,   100] loss: 0.005\n",
      "[5,   150] loss: 0.005\n",
      "[5,   200] loss: 0.005\n",
      "[5,   250] loss: 0.005\n",
      "[5,   300] loss: 0.005\n",
      "[5,   350] loss: 0.005\n",
      "[5,   400] loss: 0.005\n",
      "[5,   450] loss: 0.005\n",
      "[5,   500] loss: 0.005\n",
      "[5,   550] loss: 0.005\n",
      "[5,   600] loss: 0.005\n",
      "[5,   650] loss: 0.005\n",
      "[5,   700] loss: 0.005\n",
      "[5,   750] loss: 0.005\n",
      "[5,   800] loss: 0.005\n",
      "[5,   850] loss: 0.005\n",
      "[5,   900] loss: 0.005\n",
      "[5,   950] loss: 0.005\n",
      "[5,  1000] loss: 0.005\n",
      "[5,  1050] loss: 0.005\n",
      "[5,  1100] loss: 0.005\n",
      "[5,  1150] loss: 0.005\n",
      "[5,  1200] loss: 0.005\n",
      "[5,  1250] loss: 0.005\n",
      "[5,  1300] loss: 0.005\n",
      "[5,  1350] loss: 0.005\n",
      "[5,  1400] loss: 0.005\n",
      "[5,  1450] loss: 0.005\n",
      "[5,  1500] loss: 0.005\n",
      "[5,  1550] loss: 0.005\n",
      "[5,  1600] loss: 0.005\n",
      "[5,  1650] loss: 0.005\n",
      "[5,  1700] loss: 0.005\n",
      "[5,  1750] loss: 0.005\n",
      "[5,  1800] loss: 0.005\n",
      "[5,  1850] loss: 0.005\n",
      "[5,  1900] loss: 0.005\n",
      "[5,  1950] loss: 0.005\n",
      "[5,  2000] loss: 0.005\n",
      "[5,  2050] loss: 0.005\n",
      "[5,  2100] loss: 0.005\n",
      "[5,  2150] loss: 0.005\n",
      "[5,  2200] loss: 0.005\n",
      "[5,  2250] loss: 0.005\n",
      "[5,  2300] loss: 0.005\n",
      "[5,  2350] loss: 0.005\n",
      "[5,  2400] loss: 0.005\n",
      "[5,  2450] loss: 0.005\n",
      "[5,  2500] loss: 0.005\n",
      "[5,  2550] loss: 0.005\n",
      "[5,  2600] loss: 0.005\n",
      "[5,  2650] loss: 0.005\n",
      "[5,  2700] loss: 0.005\n",
      "[5,  2750] loss: 0.005\n",
      "[5,  2800] loss: 0.005\n",
      "[5,  2850] loss: 0.005\n",
      "[5,  2900] loss: 0.005\n",
      "[5,  2950] loss: 0.005\n",
      "[5,  3000] loss: 0.005\n",
      "[5,  3050] loss: 0.005\n",
      "[5,  3100] loss: 0.006\n",
      "[5,  3150] loss: 0.005\n",
      "[5,  3200] loss: 0.005\n",
      "[5,  3250] loss: 0.005\n",
      "[5,  3300] loss: 0.005\n",
      "[5,  3350] loss: 0.005\n",
      "[5,  3400] loss: 0.005\n",
      "[5,  3450] loss: 0.005\n",
      "[5,  3500] loss: 0.006\n",
      "[5,  3550] loss: 0.004\n",
      "[5,  3600] loss: 0.005\n",
      "[5,  3650] loss: 0.005\n",
      "[5,  3700] loss: 0.005\n",
      "[6,    50] loss: 0.005\n",
      "[6,   100] loss: 0.005\n",
      "[6,   150] loss: 0.005\n",
      "[6,   200] loss: 0.005\n",
      "[6,   250] loss: 0.005\n",
      "[6,   300] loss: 0.005\n",
      "[6,   350] loss: 0.005\n",
      "[6,   400] loss: 0.005\n",
      "[6,   450] loss: 0.004\n",
      "[6,   500] loss: 0.005\n",
      "[6,   550] loss: 0.005\n",
      "[6,   600] loss: 0.005\n",
      "[6,   650] loss: 0.004\n",
      "[6,   700] loss: 0.005\n",
      "[6,   750] loss: 0.005\n",
      "[6,   800] loss: 0.005\n",
      "[6,   850] loss: 0.005\n",
      "[6,   900] loss: 0.005\n",
      "[6,   950] loss: 0.005\n",
      "[6,  1000] loss: 0.005\n",
      "[6,  1050] loss: 0.005\n",
      "[6,  1100] loss: 0.005\n",
      "[6,  1150] loss: 0.005\n",
      "[6,  1200] loss: 0.005\n",
      "[6,  1250] loss: 0.005\n",
      "[6,  1300] loss: 0.005\n",
      "[6,  1350] loss: 0.005\n",
      "[6,  1400] loss: 0.005\n",
      "[6,  1450] loss: 0.005\n",
      "[6,  1500] loss: 0.004\n",
      "[6,  1550] loss: 0.005\n",
      "[6,  1600] loss: 0.005\n",
      "[6,  1650] loss: 0.005\n",
      "[6,  1700] loss: 0.005\n",
      "[6,  1750] loss: 0.004\n",
      "[6,  1800] loss: 0.005\n",
      "[6,  1850] loss: 0.005\n",
      "[6,  1900] loss: 0.005\n",
      "[6,  1950] loss: 0.005\n",
      "[6,  2000] loss: 0.005\n",
      "[6,  2050] loss: 0.005\n",
      "[6,  2100] loss: 0.005\n",
      "[6,  2150] loss: 0.005\n",
      "[6,  2200] loss: 0.005\n",
      "[6,  2250] loss: 0.005\n",
      "[6,  2300] loss: 0.005\n",
      "[6,  2350] loss: 0.005\n",
      "[6,  2400] loss: 0.005\n",
      "[6,  2450] loss: 0.005\n",
      "[6,  2500] loss: 0.005\n",
      "[6,  2550] loss: 0.005\n",
      "[6,  2600] loss: 0.005\n",
      "[6,  2650] loss: 0.005\n",
      "[6,  2700] loss: 0.005\n",
      "[6,  2750] loss: 0.005\n",
      "[6,  2800] loss: 0.005\n",
      "[6,  2850] loss: 0.005\n",
      "[6,  2900] loss: 0.005\n",
      "[6,  2950] loss: 0.005\n",
      "[6,  3000] loss: 0.005\n",
      "[6,  3050] loss: 0.005\n",
      "[6,  3100] loss: 0.005\n",
      "[6,  3150] loss: 0.005\n",
      "[6,  3200] loss: 0.005\n",
      "[6,  3250] loss: 0.005\n",
      "[6,  3300] loss: 0.005\n",
      "[6,  3350] loss: 0.005\n",
      "[6,  3400] loss: 0.005\n",
      "[6,  3450] loss: 0.005\n",
      "[6,  3500] loss: 0.005\n",
      "[6,  3550] loss: 0.004\n",
      "[6,  3600] loss: 0.005\n",
      "[6,  3650] loss: 0.005\n",
      "[6,  3700] loss: 0.005\n",
      "[7,    50] loss: 0.005\n",
      "[7,   100] loss: 0.005\n",
      "[7,   150] loss: 0.005\n",
      "[7,   200] loss: 0.005\n",
      "[7,   250] loss: 0.005\n",
      "[7,   300] loss: 0.005\n",
      "[7,   350] loss: 0.005\n",
      "[7,   400] loss: 0.005\n",
      "[7,   450] loss: 0.004\n",
      "[7,   500] loss: 0.005\n",
      "[7,   550] loss: 0.005\n",
      "[7,   600] loss: 0.005\n",
      "[7,   650] loss: 0.005\n",
      "[7,   700] loss: 0.004\n",
      "[7,   750] loss: 0.005\n",
      "[7,   800] loss: 0.005\n",
      "[7,   850] loss: 0.005\n",
      "[7,   900] loss: 0.005\n",
      "[7,   950] loss: 0.005\n",
      "[7,  1000] loss: 0.005\n",
      "[7,  1050] loss: 0.004\n",
      "[7,  1100] loss: 0.004\n",
      "[7,  1150] loss: 0.005\n",
      "[7,  1200] loss: 0.005\n",
      "[7,  1250] loss: 0.005\n",
      "[7,  1300] loss: 0.005\n",
      "[7,  1350] loss: 0.005\n",
      "[7,  1400] loss: 0.005\n",
      "[7,  1450] loss: 0.005\n",
      "[7,  1500] loss: 0.004\n",
      "[7,  1550] loss: 0.005\n",
      "[7,  1600] loss: 0.005\n",
      "[7,  1650] loss: 0.005\n",
      "[7,  1700] loss: 0.005\n",
      "[7,  1750] loss: 0.004\n",
      "[7,  1800] loss: 0.004\n",
      "[7,  1850] loss: 0.005\n",
      "[7,  1900] loss: 0.005\n",
      "[7,  1950] loss: 0.005\n",
      "[7,  2000] loss: 0.005\n",
      "[7,  2050] loss: 0.005\n",
      "[7,  2100] loss: 0.005\n",
      "[7,  2150] loss: 0.005\n",
      "[7,  2200] loss: 0.004\n",
      "[7,  2250] loss: 0.005\n",
      "[7,  2300] loss: 0.005\n",
      "[7,  2350] loss: 0.005\n",
      "[7,  2400] loss: 0.005\n",
      "[7,  2450] loss: 0.005\n",
      "[7,  2500] loss: 0.005\n",
      "[7,  2550] loss: 0.005\n",
      "[7,  2600] loss: 0.005\n",
      "[7,  2650] loss: 0.005\n",
      "[7,  2700] loss: 0.005\n",
      "[7,  2750] loss: 0.005\n",
      "[7,  2800] loss: 0.005\n",
      "[7,  2850] loss: 0.005\n",
      "[7,  2900] loss: 0.005\n",
      "[7,  2950] loss: 0.005\n",
      "[7,  3000] loss: 0.005\n",
      "[7,  3050] loss: 0.005\n",
      "[7,  3100] loss: 0.005\n",
      "[7,  3150] loss: 0.005\n",
      "[7,  3200] loss: 0.005\n",
      "[7,  3250] loss: 0.005\n",
      "[7,  3300] loss: 0.005\n",
      "[7,  3350] loss: 0.005\n",
      "[7,  3400] loss: 0.004\n",
      "[7,  3450] loss: 0.005\n",
      "[7,  3500] loss: 0.005\n",
      "[7,  3550] loss: 0.004\n",
      "[7,  3600] loss: 0.005\n",
      "[7,  3650] loss: 0.005\n",
      "[7,  3700] loss: 0.005\n",
      "[8,    50] loss: 0.005\n",
      "[8,   100] loss: 0.005\n",
      "[8,   150] loss: 0.004\n",
      "[8,   200] loss: 0.005\n",
      "[8,   250] loss: 0.004\n",
      "[8,   300] loss: 0.005\n",
      "[8,   350] loss: 0.004\n",
      "[8,   400] loss: 0.005\n",
      "[8,   450] loss: 0.004\n",
      "[8,   500] loss: 0.005\n",
      "[8,   550] loss: 0.005\n",
      "[8,   600] loss: 0.005\n",
      "[8,   650] loss: 0.005\n",
      "[8,   700] loss: 0.004\n",
      "[8,   750] loss: 0.005\n",
      "[8,   800] loss: 0.004\n",
      "[8,   850] loss: 0.005\n",
      "[8,   900] loss: 0.005\n",
      "[8,   950] loss: 0.005\n",
      "[8,  1000] loss: 0.004\n",
      "[8,  1050] loss: 0.005\n",
      "[8,  1100] loss: 0.004\n",
      "[8,  1150] loss: 0.005\n",
      "[8,  1200] loss: 0.005\n",
      "[8,  1250] loss: 0.004\n",
      "[8,  1300] loss: 0.005\n",
      "[8,  1350] loss: 0.005\n",
      "[8,  1400] loss: 0.005\n",
      "[8,  1450] loss: 0.005\n",
      "[8,  1500] loss: 0.004\n",
      "[8,  1550] loss: 0.005\n",
      "[8,  1600] loss: 0.005\n",
      "[8,  1650] loss: 0.005\n",
      "[8,  1700] loss: 0.004\n",
      "[8,  1750] loss: 0.004\n",
      "[8,  1800] loss: 0.004\n",
      "[8,  1850] loss: 0.005\n",
      "[8,  1900] loss: 0.005\n",
      "[8,  1950] loss: 0.004\n",
      "[8,  2000] loss: 0.005\n",
      "[8,  2050] loss: 0.005\n",
      "[8,  2100] loss: 0.005\n",
      "[8,  2150] loss: 0.005\n",
      "[8,  2200] loss: 0.004\n",
      "[8,  2250] loss: 0.005\n",
      "[8,  2300] loss: 0.005\n",
      "[8,  2350] loss: 0.005\n",
      "[8,  2400] loss: 0.005\n",
      "[8,  2450] loss: 0.005\n",
      "[8,  2500] loss: 0.004\n",
      "[8,  2550] loss: 0.005\n",
      "[8,  2600] loss: 0.005\n",
      "[8,  2650] loss: 0.005\n",
      "[8,  2700] loss: 0.005\n",
      "[8,  2750] loss: 0.004\n",
      "[8,  2800] loss: 0.005\n",
      "[8,  2850] loss: 0.005\n",
      "[8,  2900] loss: 0.004\n",
      "[8,  2950] loss: 0.004\n",
      "[8,  3000] loss: 0.005\n",
      "[8,  3050] loss: 0.005\n",
      "[8,  3100] loss: 0.005\n",
      "[8,  3150] loss: 0.005\n",
      "[8,  3200] loss: 0.005\n",
      "[8,  3250] loss: 0.004\n",
      "[8,  3300] loss: 0.004\n",
      "[8,  3350] loss: 0.004\n",
      "[8,  3400] loss: 0.004\n",
      "[8,  3450] loss: 0.005\n",
      "[8,  3500] loss: 0.005\n",
      "[8,  3550] loss: 0.004\n",
      "[8,  3600] loss: 0.005\n",
      "[8,  3650] loss: 0.005\n",
      "[8,  3700] loss: 0.005\n",
      "[9,    50] loss: 0.004\n",
      "[9,   100] loss: 0.005\n",
      "[9,   150] loss: 0.004\n",
      "[9,   200] loss: 0.004\n",
      "[9,   250] loss: 0.004\n",
      "[9,   300] loss: 0.004\n",
      "[9,   350] loss: 0.004\n",
      "[9,   400] loss: 0.005\n",
      "[9,   450] loss: 0.004\n",
      "[9,   500] loss: 0.005\n",
      "[9,   550] loss: 0.005\n",
      "[9,   600] loss: 0.005\n",
      "[9,   650] loss: 0.004\n",
      "[9,   700] loss: 0.004\n",
      "[9,   750] loss: 0.004\n",
      "[9,   800] loss: 0.005\n",
      "[9,   850] loss: 0.004\n",
      "[9,   900] loss: 0.005\n",
      "[9,   950] loss: 0.004\n",
      "[9,  1000] loss: 0.004\n",
      "[9,  1050] loss: 0.004\n",
      "[9,  1100] loss: 0.004\n",
      "[9,  1150] loss: 0.004\n",
      "[9,  1200] loss: 0.005\n",
      "[9,  1250] loss: 0.005\n",
      "[9,  1300] loss: 0.005\n",
      "[9,  1350] loss: 0.005\n",
      "[9,  1400] loss: 0.005\n",
      "[9,  1450] loss: 0.004\n",
      "[9,  1500] loss: 0.004\n",
      "[9,  1550] loss: 0.004\n",
      "[9,  1600] loss: 0.005\n",
      "[9,  1650] loss: 0.005\n",
      "[9,  1700] loss: 0.004\n",
      "[9,  1750] loss: 0.004\n",
      "[9,  1800] loss: 0.004\n",
      "[9,  1850] loss: 0.004\n",
      "[9,  1900] loss: 0.005\n",
      "[9,  1950] loss: 0.004\n",
      "[9,  2000] loss: 0.005\n",
      "[9,  2050] loss: 0.005\n",
      "[9,  2100] loss: 0.005\n",
      "[9,  2150] loss: 0.004\n",
      "[9,  2200] loss: 0.004\n",
      "[9,  2250] loss: 0.005\n",
      "[9,  2300] loss: 0.005\n",
      "[9,  2350] loss: 0.004\n",
      "[9,  2400] loss: 0.005\n",
      "[9,  2450] loss: 0.004\n",
      "[9,  2500] loss: 0.004\n",
      "[9,  2550] loss: 0.005\n",
      "[9,  2600] loss: 0.005\n",
      "[9,  2650] loss: 0.004\n",
      "[9,  2700] loss: 0.004\n",
      "[9,  2750] loss: 0.004\n",
      "[9,  2800] loss: 0.004\n",
      "[9,  2850] loss: 0.004\n",
      "[9,  2900] loss: 0.004\n",
      "[9,  2950] loss: 0.004\n",
      "[9,  3000] loss: 0.005\n",
      "[9,  3050] loss: 0.005\n",
      "[9,  3100] loss: 0.005\n",
      "[9,  3150] loss: 0.004\n",
      "[9,  3200] loss: 0.005\n",
      "[9,  3250] loss: 0.004\n",
      "[9,  3300] loss: 0.004\n",
      "[9,  3350] loss: 0.004\n",
      "[9,  3400] loss: 0.004\n",
      "[9,  3450] loss: 0.005\n",
      "[9,  3500] loss: 0.005\n",
      "[9,  3550] loss: 0.004\n",
      "[9,  3600] loss: 0.005\n",
      "[9,  3650] loss: 0.005\n",
      "[9,  3700] loss: 0.004\n",
      "[10,    50] loss: 0.004\n",
      "[10,   100] loss: 0.005\n",
      "[10,   150] loss: 0.004\n",
      "[10,   200] loss: 0.005\n",
      "[10,   250] loss: 0.004\n",
      "[10,   300] loss: 0.005\n",
      "[10,   350] loss: 0.004\n",
      "[10,   400] loss: 0.005\n",
      "[10,   450] loss: 0.004\n",
      "[10,   500] loss: 0.005\n",
      "[10,   550] loss: 0.005\n",
      "[10,   600] loss: 0.005\n",
      "[10,   650] loss: 0.004\n",
      "[10,   700] loss: 0.004\n",
      "[10,   750] loss: 0.004\n",
      "[10,   800] loss: 0.004\n",
      "[10,   850] loss: 0.004\n",
      "[10,   900] loss: 0.004\n",
      "[10,   950] loss: 0.004\n",
      "[10,  1000] loss: 0.004\n",
      "[10,  1050] loss: 0.004\n",
      "[10,  1100] loss: 0.004\n",
      "[10,  1150] loss: 0.004\n",
      "[10,  1200] loss: 0.005\n",
      "[10,  1250] loss: 0.004\n",
      "[10,  1300] loss: 0.004\n",
      "[10,  1350] loss: 0.005\n",
      "[10,  1400] loss: 0.004\n",
      "[10,  1450] loss: 0.004\n",
      "[10,  1500] loss: 0.004\n",
      "[10,  1550] loss: 0.004\n",
      "[10,  1600] loss: 0.005\n",
      "[10,  1650] loss: 0.005\n",
      "[10,  1700] loss: 0.004\n",
      "[10,  1750] loss: 0.004\n",
      "[10,  1800] loss: 0.004\n",
      "[10,  1850] loss: 0.004\n",
      "[10,  1900] loss: 0.005\n",
      "[10,  1950] loss: 0.004\n",
      "[10,  2000] loss: 0.004\n",
      "[10,  2050] loss: 0.004\n",
      "[10,  2100] loss: 0.004\n",
      "[10,  2150] loss: 0.004\n",
      "[10,  2200] loss: 0.004\n",
      "[10,  2250] loss: 0.005\n",
      "[10,  2300] loss: 0.005\n",
      "[10,  2350] loss: 0.004\n",
      "[10,  2400] loss: 0.004\n",
      "[10,  2450] loss: 0.004\n",
      "[10,  2500] loss: 0.004\n",
      "[10,  2550] loss: 0.004\n",
      "[10,  2600] loss: 0.004\n",
      "[10,  2650] loss: 0.004\n",
      "[10,  2700] loss: 0.004\n",
      "[10,  2750] loss: 0.005\n",
      "[10,  2800] loss: 0.004\n",
      "[10,  2850] loss: 0.005\n",
      "[10,  2900] loss: 0.004\n",
      "[10,  2950] loss: 0.004\n",
      "[10,  3000] loss: 0.005\n",
      "[10,  3050] loss: 0.005\n",
      "[10,  3100] loss: 0.005\n",
      "[10,  3150] loss: 0.004\n",
      "[10,  3200] loss: 0.005\n",
      "[10,  3250] loss: 0.004\n",
      "[10,  3300] loss: 0.004\n",
      "[10,  3350] loss: 0.004\n",
      "[10,  3400] loss: 0.004\n",
      "[10,  3450] loss: 0.005\n",
      "[10,  3500] loss: 0.005\n",
      "[10,  3550] loss: 0.004\n",
      "[10,  3600] loss: 0.005\n",
      "[10,  3650] loss: 0.005\n",
      "[10,  3700] loss: 0.004\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # print(inputs[0,:,:].shape)\n",
    "        # print(outputs[0,:,:].shape)\n",
    "        # print(labels[0,:,:].shape)\n",
    "        \n",
    "        # print(inputs.shape)\n",
    "        # print(outputs.shape)\n",
    "        # print(labels.shape)\n",
    "        \n",
    "        outputs = (torch.flatten(outputs, start_dim=0, end_dim=1))\n",
    "        labels = (torch.flatten(labels, start_dim=0, end_dim=1))\n",
    "        \n",
    "        # break\n",
    "        # outputs = torch.transpose(outputs,1,2)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # for name, param in model.named_parameters():\n",
    "        #     print(name, param.grad)\n",
    "\n",
    "        # if(i > 2):\n",
    "        #     break\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 50 == 49:    # print every 50 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"MODEL_V0.pth\"\n",
    "MODEL_SAVE_PATH = \"./models/\"+MODEL_NAME\n",
    "\n",
    "torch.save(obj=model.state_dict(), f=MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  90.9375\n",
      "Accuracy:  84.6875\n",
      "Accuracy:  91.25\n",
      "Accuracy:  95.9375\n",
      "Accuracy:  93.125\n",
      "Accuracy:  95.9375\n",
      "Accuracy:  93.125\n",
      "Accuracy:  88.75\n",
      "Accuracy:  89.6875\n",
      "Accuracy:  92.1875\n",
      "Accuracy:  92.8125\n",
      "Accuracy:  88.4375\n",
      "Accuracy:  92.8125\n",
      "Accuracy:  87.8125\n",
      "Accuracy:  92.5\n",
      "Accuracy:  90.3125\n",
      "Accuracy:  95.0\n",
      "Accuracy:  91.875\n",
      "Accuracy:  89.375\n",
      "Accuracy:  94.6875\n",
      "Accuracy:  94.6875\n",
      "Accuracy:  93.4375\n",
      "Accuracy:  88.125\n",
      "Accuracy:  95.3125\n",
      "Accuracy:  98.4375\n",
      "Accuracy:  91.875\n",
      "Accuracy:  90.9375\n",
      "Accuracy:  92.5\n",
      "Accuracy:  89.6875\n",
      "Accuracy:  95.0\n",
      "Accuracy:  90.3125\n",
      "Accuracy:  90.625\n",
      "Accuracy:  93.125\n",
      "Accuracy:  93.125\n",
      "Accuracy:  88.4375\n",
      "Accuracy:  93.4375\n",
      "Accuracy:  92.1875\n",
      "Accuracy:  95.625\n",
      "Accuracy:  88.4375\n",
      "Accuracy:  93.125\n",
      "Accuracy:  93.75\n",
      "Accuracy:  95.0\n",
      "Accuracy:  93.75\n",
      "Accuracy:  98.75\n",
      "Accuracy:  89.6875\n",
      "Accuracy:  85.9375\n",
      "Accuracy:  95.0\n",
      "Accuracy:  95.0\n",
      "Accuracy:  91.875\n",
      "Accuracy:  93.4375\n",
      "Accuracy:  92.8125\n",
      "Accuracy:  93.4375\n",
      "Accuracy:  91.25\n",
      "Accuracy:  91.25\n",
      "Accuracy:  94.6875\n",
      "Accuracy:  96.875\n",
      "Accuracy:  85.625\n",
      "Accuracy:  92.8125\n",
      "Accuracy:  90.3125\n",
      "Accuracy:  93.125\n",
      "Accuracy:  96.25\n",
      "Accuracy:  95.0\n",
      "Accuracy:  87.1875\n",
      "Accuracy:  90.9375\n",
      "Accuracy:  90.9375\n",
      "Accuracy:  93.4375\n",
      "Accuracy:  93.4375\n",
      "Accuracy:  89.375\n",
      "Accuracy:  92.5\n",
      "Accuracy:  94.0625\n",
      "Accuracy:  92.5\n",
      "Accuracy:  99.6875\n",
      "Accuracy:  91.875\n",
      "Accuracy:  95.0\n",
      "Accuracy:  93.125\n",
      "Accuracy:  88.4375\n",
      "Accuracy:  88.75\n",
      "Accuracy:  87.8125\n",
      "Accuracy:  95.625\n",
      "Accuracy:  93.125\n",
      "Accuracy:  89.375\n",
      "Accuracy:  94.6875\n",
      "Accuracy:  89.375\n",
      "Accuracy:  94.0625\n",
      "Accuracy:  88.75\n",
      "Accuracy:  96.25\n",
      "Accuracy:  92.8125\n",
      "Accuracy:  88.125\n",
      "Accuracy:  97.1875\n",
      "Accuracy:  92.8125\n",
      "Accuracy:  85.9375\n",
      "Accuracy:  92.8125\n",
      "Accuracy:  95.0\n",
      "Accuracy:  91.5625\n",
      "Accuracy:  90.0\n",
      "Accuracy:  84.375\n",
      "Accuracy:  94.0625\n",
      "Accuracy:  89.375\n",
      "Accuracy:  92.8125\n",
      "Accuracy:  91.875\n",
      "Accuracy:  94.375\n",
      "Accuracy:  91.5625\n",
      "Accuracy:  93.125\n",
      "Accuracy:  95.3125\n",
      "Accuracy:  92.5\n",
      "Accuracy:  86.5625\n",
      "Accuracy:  84.0625\n",
      "Accuracy:  95.625\n",
      "Accuracy:  91.875\n",
      "Accuracy:  90.0\n",
      "Accuracy:  94.375\n",
      "Accuracy:  94.375\n",
      "Accuracy:  90.625\n",
      "Accuracy:  93.125\n",
      "Accuracy:  94.6875\n",
      "Accuracy:  93.4375\n",
      "Accuracy:  92.1875\n",
      "Accuracy:  91.875\n",
      "Accuracy:  89.0625\n",
      "Accuracy:  94.6875\n",
      "Accuracy:  96.5625\n",
      "Accuracy:  96.5625\n",
      "Accuracy:  86.25\n",
      "Accuracy:  93.4375\n",
      "Accuracy:  89.375\n",
      "Accuracy:  85.9375\n",
      "Accuracy:  96.25\n",
      "Accuracy:  96.875\n",
      "Accuracy:  90.9375\n",
      "Accuracy:  87.8125\n",
      "Accuracy:  90.625\n",
      "Accuracy:  89.375\n",
      "Accuracy:  91.875\n",
      "Accuracy:  90.3125\n",
      "Accuracy:  94.375\n",
      "Accuracy:  93.125\n",
      "Accuracy:  87.8125\n",
      "Accuracy:  91.5625\n",
      "Accuracy:  93.125\n",
      "Accuracy:  94.375\n",
      "Accuracy:  90.0\n",
      "Accuracy:  95.9375\n",
      "Accuracy:  90.625\n",
      "Accuracy:  90.625\n",
      "Accuracy:  94.375\n",
      "Accuracy:  91.25\n",
      "Accuracy:  88.4375\n",
      "Accuracy:  93.125\n",
      "Accuracy:  94.6875\n",
      "Accuracy:  93.75\n",
      "Accuracy:  94.6875\n",
      "Accuracy:  95.625\n",
      "Accuracy:  90.9375\n",
      "Accuracy:  89.375\n",
      "Accuracy:  93.75\n",
      "Accuracy:  94.0625\n",
      "Accuracy:  89.0625\n",
      "Accuracy:  92.5\n",
      "Accuracy:  94.375\n",
      "Accuracy:  88.125\n",
      "Accuracy:  91.875\n",
      "Accuracy:  88.4375\n",
      "Accuracy:  93.125\n",
      "Accuracy:  91.25\n",
      "Accuracy:  93.4375\n",
      "Accuracy:  96.875\n",
      "Accuracy:  91.25\n",
      "Accuracy:  92.5\n",
      "Accuracy:  90.0\n",
      "Accuracy:  91.5625\n",
      "Accuracy:  97.5\n",
      "Accuracy:  90.3125\n",
      "Accuracy:  94.375\n",
      "Accuracy:  85.625\n",
      "Accuracy:  91.25\n",
      "Accuracy:  95.0\n",
      "Accuracy:  92.5\n",
      "Accuracy:  93.125\n",
      "Accuracy:  91.875\n",
      "Accuracy:  93.75\n",
      "Accuracy:  89.6875\n",
      "Accuracy:  91.5625\n",
      "Accuracy:  94.0625\n",
      "Accuracy:  92.1875\n",
      "Accuracy:  97.1875\n",
      "Accuracy:  91.875\n",
      "Accuracy:  89.0625\n",
      "Accuracy:  93.4375\n",
      "Accuracy:  89.0625\n",
      "Accuracy:  94.375\n",
      "Accuracy:  88.4375\n",
      "Accuracy:  92.1875\n",
      "Accuracy:  86.25\n",
      "Accuracy:  90.9375\n",
      "Accuracy:  95.9375\n",
      "Accuracy:  90.625\n",
      "Accuracy:  88.4375\n",
      "Accuracy:  86.875\n",
      "Accuracy:  93.75\n",
      "Accuracy:  91.5625\n",
      "Accuracy:  88.4375\n",
      "Accuracy:  89.375\n",
      "Accuracy:  88.125\n",
      "Accuracy:  90.9375\n",
      "Accuracy:  96.5625\n",
      "Accuracy:  86.5625\n",
      "Accuracy:  91.5625\n",
      "Accuracy:  87.8125\n",
      "Accuracy:  88.75\n",
      "Accuracy:  90.0\n",
      "Accuracy:  97.8125\n",
      "Accuracy:  92.5\n",
      "Accuracy:  93.4375\n",
      "Accuracy:  95.3125\n",
      "Accuracy:  91.875\n",
      "Accuracy:  91.875\n",
      "Accuracy:  94.375\n",
      "Accuracy:  96.25\n",
      "Accuracy:  83.125\n",
      "Accuracy:  93.4375\n",
      "Accuracy:  94.375\n",
      "Accuracy:  90.0\n",
      "Accuracy:  94.6875\n",
      "Accuracy:  90.3125\n",
      "Accuracy:  96.5625\n",
      "Accuracy:  93.75\n",
      "Accuracy:  91.875\n",
      "Accuracy:  90.625\n",
      "Accuracy:  83.4375\n",
      "Accuracy:  96.875\n",
      "Accuracy:  94.0625\n",
      "Accuracy:  91.25\n",
      "Accuracy:  93.75\n",
      "Accuracy:  87.8125\n",
      "Accuracy:  89.375\n",
      "Accuracy:  95.9375\n",
      "Accuracy:  97.5\n",
      "Accuracy:  86.25\n",
      "Accuracy:  92.1875\n",
      "Accuracy:  92.8125\n",
      "Accuracy:  89.6875\n",
      "Accuracy:  95.9375\n",
      "Accuracy:  92.8125\n",
      "Accuracy:  91.5625\n",
      "Accuracy:  93.4375\n",
      "Accuracy:  85.625\n",
      "Accuracy:  83.125\n",
      "Accuracy:  92.8125\n",
      "Accuracy:  96.5625\n",
      "Accuracy:  93.125\n",
      "Accuracy:  95.0\n",
      "Accuracy:  93.125\n",
      "Accuracy:  85.625\n",
      "Accuracy:  93.75\n",
      "Accuracy:  93.4375\n",
      "Accuracy:  90.0\n",
      "Accuracy:  89.375\n",
      "Accuracy:  92.8125\n",
      "Accuracy:  95.3125\n",
      "Accuracy:  85.625\n",
      "Accuracy:  90.625\n",
      "Accuracy:  90.0\n",
      "Accuracy:  91.5625\n",
      "Accuracy:  90.0\n",
      "Accuracy:  86.25\n",
      "Accuracy:  93.125\n",
      "Accuracy:  93.75\n",
      "Accuracy:  90.625\n",
      "Accuracy:  87.5\n",
      "Accuracy:  96.5625\n",
      "Accuracy:  91.5625\n",
      "Accuracy:  83.4375\n",
      "Accuracy:  94.375\n",
      "Accuracy:  92.5\n",
      "Accuracy:  96.5625\n",
      "Accuracy:  91.875\n",
      "Accuracy:  93.75\n",
      "Accuracy:  93.4375\n",
      "Accuracy:  90.625\n",
      "Accuracy:  89.0625\n",
      "Accuracy:  93.4375\n",
      "Accuracy:  87.8125\n",
      "Accuracy:  95.0\n",
      "Accuracy:  95.9375\n",
      "Accuracy:  90.0\n",
      "Accuracy:  95.3125\n",
      "Accuracy:  91.5625\n",
      "Accuracy:  94.6875\n",
      "Accuracy:  93.75\n",
      "Accuracy:  86.875\n",
      "Accuracy:  90.9375\n",
      "Accuracy:  89.6875\n",
      "Accuracy:  95.625\n",
      "Accuracy:  93.75\n",
      "Accuracy:  91.25\n",
      "Accuracy:  96.875\n",
      "Accuracy:  90.9375\n",
      "Accuracy:  95.3125\n",
      "Accuracy:  94.0625\n",
      "Accuracy:  92.1875\n",
      "Accuracy:  95.9375\n",
      "Accuracy:  95.9375\n",
      "Accuracy:  93.4375\n",
      "Accuracy:  95.0\n",
      "Accuracy:  95.0\n",
      "Accuracy:  90.3125\n",
      "Accuracy:  94.0625\n",
      "Accuracy:  91.25\n",
      "Accuracy:  95.3125\n",
      "Accuracy:  93.4375\n",
      "Accuracy:  92.8125\n",
      "Accuracy:  92.5\n",
      "Accuracy:  92.8125\n",
      "Accuracy:  97.5\n",
      "Accuracy:  95.3125\n",
      "Accuracy:  95.0\n",
      "Accuracy:  90.9375\n",
      "Accuracy:  93.75\n",
      "Accuracy:  95.0\n",
      "Accuracy:  94.375\n",
      "Accuracy:  94.0625\n",
      "Accuracy:  87.1875\n",
      "Accuracy:  89.0625\n",
      "Accuracy:  85.3125\n",
      "Accuracy:  96.875\n",
      "Accuracy:  95.3125\n",
      "Accuracy:  93.75\n",
      "Accuracy:  90.625\n",
      "Accuracy:  90.3125\n",
      "Accuracy:  87.1875\n",
      "Accuracy:  93.125\n",
      "Accuracy:  95.625\n",
      "Accuracy:  97.5\n",
      "Accuracy:  86.5625\n",
      "Accuracy:  90.9375\n",
      "Accuracy:  88.125\n",
      "Accuracy:  93.125\n",
      "Accuracy:  93.4375\n",
      "Accuracy:  91.875\n",
      "Accuracy:  92.8125\n",
      "Accuracy:  98.75\n",
      "Accuracy:  92.8125\n",
      "Accuracy:  93.125\n",
      "Accuracy:  91.875\n",
      "Accuracy:  87.5\n",
      "Accuracy:  89.0625\n",
      "Accuracy:  95.3125\n",
      "Accuracy:  93.75\n",
      "Accuracy:  82.8125\n",
      "Accuracy:  93.75\n",
      "Accuracy:  88.4375\n",
      "Accuracy:  92.1875\n",
      "Accuracy:  86.5625\n",
      "Accuracy:  91.875\n",
      "Accuracy:  93.4375\n",
      "Accuracy:  91.875\n",
      "Accuracy:  84.375\n",
      "Accuracy:  95.0\n",
      "Accuracy:  92.8125\n",
      "Accuracy:  99.375\n",
      "Accuracy:  93.125\n",
      "Accuracy:  93.125\n",
      "Accuracy:  91.25\n",
      "Accuracy:  88.4375\n",
      "Accuracy:  96.5625\n",
      "Accuracy:  95.0\n",
      "Accuracy:  96.25\n",
      "Accuracy:  95.0\n",
      "Accuracy:  92.1875\n",
      "Accuracy:  94.6875\n",
      "Accuracy:  95.625\n",
      "Accuracy:  95.3125\n",
      "Accuracy:  93.75\n",
      "Accuracy:  87.5\n",
      "Accuracy:  89.375\n",
      "Accuracy:  93.125\n",
      "Accuracy:  90.3125\n",
      "Accuracy:  95.9375\n",
      "Accuracy:  92.5\n",
      "Accuracy:  98.125\n",
      "Accuracy:  92.5\n",
      "Accuracy:  89.6875\n",
      "Accuracy:  91.5625\n",
      "Accuracy:  93.125\n",
      "Accuracy:  85.0\n",
      "Accuracy:  91.25\n",
      "Accuracy:  89.0625\n",
      "Accuracy:  97.1875\n",
      "Accuracy:  90.625\n",
      "Accuracy:  95.625\n",
      "Accuracy:  92.8125\n",
      "Accuracy:  88.125\n",
      "Accuracy:  91.875\n",
      "Accuracy:  84.375\n",
      "Accuracy:  97.8125\n",
      "Accuracy:  89.6875\n",
      "Accuracy:  96.25\n",
      "Accuracy:  94.6875\n",
      "Accuracy:  93.125\n",
      "Accuracy:  94.375\n",
      "Accuracy:  96.25\n",
      "Accuracy:  86.875\n",
      "Accuracy:  94.0625\n",
      "Accuracy:  92.8125\n",
      "Accuracy:  89.6875\n",
      "Accuracy:  91.875\n",
      "Accuracy:  98.125\n",
      "Accuracy:  90.625\n",
      "Accuracy:  88.75\n",
      "Accuracy:  93.75\n",
      "Accuracy:  93.75\n",
      "Accuracy:  94.375\n",
      "Accuracy:  90.9375\n",
      "Accuracy:  89.0625\n",
      "Accuracy:  90.3125\n",
      "Accuracy:  85.0\n",
      "Accuracy:  91.25\n",
      "Accuracy:  95.625\n",
      "Accuracy:  82.1875\n",
      "Accuracy:  92.1875\n",
      "Accuracy:  94.6875\n",
      "Accuracy:  94.6875\n",
      "Accuracy:  93.4375\n",
      "Accuracy:  94.0625\n",
      "Accuracy:  96.5625\n",
      "Accuracy:  85.0\n",
      "Accuracy:  89.6875\n",
      "Accuracy:  90.9375\n",
      "Accuracy:  94.6875\n",
      "Accuracy:  95.0\n",
      "Accuracy:  94.0625\n",
      "Accuracy:  90.9375\n",
      "Accuracy:  89.375\n",
      "Accuracy:  93.4375\n",
      "Accuracy:  89.6875\n",
      "Accuracy:  94.375\n",
      "Accuracy:  98.75\n",
      "Accuracy:  95.9375\n",
      "Accuracy:  96.25\n",
      "Accuracy:  98.125\n",
      "Accuracy:  85.625\n",
      "Accuracy:  95.625\n",
      "Accuracy:  90.0\n",
      "Accuracy:  92.1875\n",
      "Accuracy:  96.875\n",
      "Accuracy:  93.4375\n",
      "Accuracy:  94.0625\n",
      "Accuracy:  94.0625\n",
      "Accuracy:  93.4375\n",
      "Accuracy:  93.75\n",
      "Accuracy:  95.0\n",
      "Accuracy:  92.5\n",
      "Accuracy:  87.8125\n",
      "Accuracy:  96.25\n",
      "Accuracy:  94.0625\n",
      "Accuracy:  92.1875\n",
      "Accuracy:  94.375\n",
      "Accuracy:  86.875\n",
      "Accuracy:  88.125\n",
      "Accuracy:  95.625\n",
      "Accuracy:  93.75\n",
      "Accuracy:  85.3125\n",
      "Accuracy:  93.4375\n",
      "Accuracy:  98.125\n",
      "Accuracy:  89.6875\n",
      "Accuracy:  84.6875\n",
      "Accuracy:  97.1875\n",
      "Accuracy:  90.9375\n",
      "Accuracy:  92.8125\n",
      "Accuracy:  94.375\n",
      "Accuracy:  90.3125\n",
      "Accuracy:  93.75\n",
      "Accuracy:  89.0625\n",
      "Accuracy:  91.875\n",
      "Accuracy:  91.875\n",
      "Accuracy:  89.6875\n",
      "Accuracy:  93.75\n",
      "Accuracy:  92.1875\n",
      "Accuracy:  89.0625\n",
      "Accuracy:  96.875\n",
      "Accuracy:  95.0\n",
      "Accuracy:  93.75\n",
      "Accuracy:  93.75\n",
      "Accuracy:  90.0\n",
      "Accuracy:  90.9375\n",
      "Accuracy:  86.875\n",
      "Accuracy:  88.4375\n",
      "Accuracy:  94.6875\n",
      "Accuracy:  90.625\n",
      "Accuracy:  92.5\n",
      "Accuracy:  92.8125\n",
      "Accuracy:  89.375\n",
      "Accuracy:  89.375\n",
      "Accuracy:  89.375\n",
      "Accuracy:  93.75\n",
      "Accuracy:  95.3125\n",
      "Accuracy:  91.25\n",
      "Accuracy:  94.6875\n",
      "Accuracy:  92.1875\n",
      "Accuracy:  89.0625\n",
      "Accuracy:  92.5\n",
      "Accuracy:  97.1875\n",
      "Accuracy:  88.125\n",
      "Accuracy:  95.9375\n",
      "Accuracy:  84.0625\n",
      "Accuracy:  94.6875\n",
      "Accuracy:  91.875\n",
      "Accuracy:  94.6875\n",
      "Accuracy:  94.6875\n",
      "Accuracy:  89.0625\n",
      "Accuracy:  87.1875\n",
      "Accuracy:  90.0\n",
      "Accuracy:  85.3125\n",
      "Accuracy:  93.125\n",
      "Accuracy:  90.625\n",
      "Accuracy:  89.375\n",
      "Accuracy:  92.1875\n",
      "Accuracy:  94.375\n",
      "Accuracy:  89.6875\n",
      "Accuracy:  88.75\n",
      "Accuracy:  93.125\n",
      "Accuracy:  95.0\n",
      "Accuracy:  93.4375\n",
      "Accuracy:  88.75\n",
      "Accuracy:  96.5625\n",
      "Accuracy:  93.125\n",
      "Accuracy:  92.8125\n",
      "Accuracy:  88.125\n",
      "Accuracy:  90.9375\n",
      "Accuracy:  93.75\n",
      "Accuracy:  95.9375\n",
      "Accuracy:  91.5625\n",
      "Accuracy:  95.0\n",
      "Accuracy:  94.375\n",
      "Accuracy:  95.0\n",
      "Accuracy:  94.6875\n",
      "Accuracy:  95.625\n",
      "Accuracy:  94.375\n",
      "Accuracy:  93.75\n",
      "Accuracy:  90.9375\n",
      "Accuracy:  85.9375\n",
      "Accuracy:  92.1875\n",
      "Accuracy:  96.5625\n",
      "Accuracy:  87.1875\n",
      "Accuracy:  96.25\n",
      "Accuracy:  95.9375\n",
      "Accuracy:  92.5\n",
      "Accuracy:  99.375\n",
      "Accuracy:  95.3125\n",
      "Accuracy:  90.9375\n",
      "Accuracy:  92.1875\n",
      "Accuracy:  91.5625\n",
      "Accuracy:  94.0625\n",
      "Accuracy:  90.3125\n",
      "Accuracy:  94.0625\n",
      "Accuracy:  91.5625\n",
      "Accuracy:  94.6875\n",
      "Accuracy:  88.125\n",
      "Accuracy:  93.125\n",
      "Accuracy:  95.3125\n",
      "Accuracy:  96.25\n",
      "Accuracy:  91.875\n",
      "Accuracy:  90.0\n",
      "Accuracy:  90.3125\n",
      "Accuracy:  82.8125\n",
      "Accuracy:  91.25\n",
      "Accuracy:  92.5\n",
      "Accuracy:  89.0625\n",
      "Accuracy:  93.125\n",
      "Accuracy:  92.5\n",
      "Accuracy:  94.6875\n",
      "Accuracy:  90.625\n",
      "Accuracy:  85.625\n",
      "Accuracy:  85.3125\n",
      "Accuracy:  90.9375\n",
      "Accuracy:  92.1875\n",
      "Accuracy:  97.1875\n",
      "Accuracy:  88.4375\n",
      "Accuracy:  85.0\n",
      "Accuracy:  89.375\n",
      "Accuracy:  93.125\n",
      "Accuracy:  95.0\n",
      "Accuracy:  93.125\n",
      "Accuracy:  89.6875\n",
      "Accuracy:  93.4375\n",
      "Accuracy:  86.25\n",
      "Accuracy:  93.125\n",
      "Accuracy:  86.875\n",
      "Accuracy:  90.0\n",
      "Accuracy:  88.75\n",
      "Accuracy:  91.25\n",
      "Accuracy:  94.375\n",
      "Accuracy:  95.625\n",
      "Accuracy:  89.0625\n",
      "Accuracy:  91.5625\n",
      "Accuracy:  91.5625\n",
      "Accuracy:  98.125\n",
      "Accuracy:  88.4375\n",
      "Accuracy:  94.6875\n",
      "Accuracy:  97.1875\n",
      "Accuracy:  92.1875\n",
      "Accuracy:  95.3125\n",
      "Accuracy:  92.1875\n",
      "Accuracy:  96.5625\n",
      "Accuracy:  87.8125\n",
      "Accuracy:  91.25\n",
      "Accuracy:  95.0\n",
      "Accuracy:  86.875\n",
      "Accuracy:  93.4375\n",
      "Accuracy:  90.0\n",
      "Accuracy:  95.0\n",
      "Accuracy:  89.375\n",
      "Accuracy:  90.625\n",
      "Accuracy:  93.4375\n",
      "Accuracy:  97.8125\n",
      "Accuracy:  84.0625\n",
      "Accuracy:  96.5625\n",
      "Accuracy:  93.125\n",
      "Accuracy:  87.5\n",
      "Accuracy:  81.25\n",
      "Accuracy:  90.9375\n",
      "Accuracy:  91.5625\n",
      "Accuracy:  89.375\n",
      "Accuracy:  89.6875\n",
      "Accuracy:  94.375\n",
      "Accuracy:  92.5\n",
      "Accuracy:  91.5625\n",
      "Accuracy:  91.5625\n",
      "Accuracy:  94.375\n",
      "Accuracy:  93.75\n",
      "Accuracy:  90.3125\n",
      "Accuracy:  90.9375\n",
      "Accuracy:  92.1875\n",
      "Accuracy:  85.9375\n",
      "Accuracy:  95.0\n",
      "Accuracy:  92.8125\n",
      "Accuracy:  95.625\n",
      "Accuracy:  95.9375\n",
      "Accuracy:  90.625\n",
      "Accuracy:  93.125\n",
      "Accuracy:  90.0\n",
      "Accuracy:  89.0625\n",
      "Accuracy:  97.8125\n",
      "Accuracy:  94.375\n",
      "Accuracy:  95.625\n",
      "Accuracy:  96.25\n",
      "Accuracy:  86.5625\n",
      "Accuracy:  90.0\n",
      "Accuracy:  89.375\n",
      "Accuracy:  93.4375\n",
      "Accuracy:  90.3125\n",
      "Accuracy:  86.5625\n",
      "Accuracy:  80.9375\n",
      "Accuracy:  92.1875\n",
      "Accuracy:  97.8125\n",
      "Accuracy:  92.5\n",
      "Accuracy:  89.6875\n",
      "Accuracy:  89.375\n",
      "Accuracy:  87.5\n",
      "Accuracy:  89.6875\n",
      "Accuracy:  89.6875\n",
      "Accuracy:  92.5\n",
      "Accuracy:  95.0\n",
      "Accuracy:  93.75\n",
      "Accuracy:  91.875\n",
      "Accuracy:  96.5625\n",
      "Accuracy:  94.6875\n",
      "Accuracy:  93.125\n",
      "Accuracy:  87.5\n",
      "Accuracy:  93.75\n",
      "Accuracy:  88.4375\n",
      "Accuracy:  95.9375\n",
      "Accuracy:  92.1875\n",
      "Accuracy:  86.875\n",
      "Accuracy:  85.625\n",
      "Accuracy:  96.5625\n",
      "Accuracy:  91.875\n",
      "Accuracy:  95.0\n",
      "Accuracy:  89.375\n",
      "Accuracy:  93.125\n",
      "Accuracy:  93.75\n",
      "Accuracy:  93.125\n",
      "Accuracy:  87.5\n",
      "Accuracy:  95.0\n",
      "Accuracy:  89.0625\n",
      "Accuracy:  90.625\n",
      "Accuracy:  93.125\n",
      "Accuracy:  95.9375\n",
      "Accuracy:  94.375\n",
      "Accuracy:  93.125\n",
      "Accuracy:  89.6875\n",
      "Accuracy:  92.8125\n",
      "Accuracy:  94.0625\n",
      "Accuracy:  90.3125\n",
      "Accuracy:  93.4375\n",
      "Accuracy:  96.25\n",
      "Accuracy:  94.6875\n",
      "Accuracy:  89.375\n",
      "Accuracy:  94.0625\n",
      "Accuracy:  87.5\n",
      "Accuracy:  90.9375\n",
      "Accuracy:  96.875\n",
      "Accuracy:  88.4375\n",
      "Accuracy:  96.875\n",
      "Accuracy:  93.75\n",
      "Accuracy:  92.1875\n",
      "Accuracy:  88.75\n",
      "Accuracy:  89.6875\n",
      "Accuracy:  95.9375\n",
      "Accuracy:  89.375\n",
      "Accuracy:  95.0\n",
      "Accuracy:  85.0\n",
      "Accuracy:  95.625\n",
      "Accuracy:  93.125\n",
      "Accuracy:  96.5625\n",
      "Accuracy:  94.0625\n",
      "Accuracy:  83.125\n",
      "Accuracy:  92.5\n",
      "Accuracy:  95.0\n",
      "Accuracy:  94.0625\n",
      "Accuracy:  92.8125\n",
      "Accuracy:  93.75\n",
      "Accuracy:  81.875\n",
      "Accuracy:  94.375\n",
      "Accuracy:  92.5\n",
      "Accuracy:  91.5625\n",
      "Accuracy:  90.625\n",
      "Accuracy:  90.3125\n",
      "Accuracy:  93.4375\n",
      "Accuracy:  87.5\n",
      "Accuracy:  90.3125\n",
      "Accuracy:  95.3125\n",
      "Accuracy:  94.0625\n",
      "Accuracy:  93.4375\n",
      "Accuracy:  94.375\n",
      "Accuracy:  90.9375\n",
      "Accuracy:  94.0625\n",
      "Accuracy:  92.5\n",
      "Accuracy:  89.6875\n",
      "Accuracy:  88.75\n",
      "Accuracy:  93.75\n",
      "Accuracy:  93.75\n",
      "Accuracy:  90.9375\n",
      "Accuracy:  91.5625\n",
      "Accuracy:  89.6875\n",
      "Accuracy:  97.5\n",
      "Accuracy:  92.1875\n",
      "Accuracy:  89.6875\n",
      "Accuracy:  90.0\n",
      "Accuracy:  95.625\n",
      "Accuracy:  91.5625\n",
      "Accuracy:  86.25\n",
      "Accuracy:  85.625\n",
      "Accuracy:  91.875\n",
      "Accuracy:  90.9375\n",
      "Accuracy:  88.125\n",
      "Accuracy:  90.625\n",
      "Accuracy:  96.5625\n",
      "Accuracy:  94.6875\n",
      "Accuracy:  92.1875\n",
      "Accuracy:  90.0\n",
      "Accuracy:  96.25\n",
      "Accuracy:  89.375\n",
      "Accuracy:  95.3125\n",
      "Accuracy:  95.0\n",
      "Accuracy:  91.5625\n",
      "Accuracy:  91.875\n",
      "Accuracy:  92.1875\n",
      "Accuracy:  89.6875\n",
      "Accuracy:  92.8125\n",
      "Accuracy:  91.25\n",
      "Accuracy:  84.375\n",
      "Accuracy:  95.0\n",
      "Accuracy:  96.25\n",
      "Accuracy:  95.9375\n",
      "Accuracy:  95.0\n",
      "Accuracy:  89.375\n",
      "Accuracy:  91.875\n",
      "Accuracy:  94.375\n",
      "Accuracy:  89.0625\n",
      "Accuracy:  88.125\n",
      "Accuracy:  88.125\n",
      "Accuracy:  93.75\n",
      "Accuracy:  90.0\n",
      "Accuracy:  94.0625\n",
      "Accuracy:  93.75\n",
      "Accuracy:  96.25\n",
      "Accuracy:  92.8125\n",
      "Accuracy:  93.75\n",
      "Accuracy:  94.375\n",
      "Accuracy:  92.1875\n",
      "Accuracy:  88.75\n",
      "Accuracy:  91.25\n",
      "Accuracy:  96.875\n",
      "Accuracy:  85.9375\n",
      "Accuracy:  91.5625\n",
      "Accuracy:  88.75\n",
      "Accuracy:  95.625\n",
      "Accuracy:  86.5625\n",
      "Accuracy:  90.3125\n",
      "Accuracy:  90.0\n",
      "Accuracy:  94.0625\n",
      "Accuracy:  93.75\n",
      "Accuracy:  91.5625\n",
      "Accuracy:  94.375\n",
      "Accuracy:  90.3125\n",
      "Accuracy:  95.625\n",
      "Accuracy:  88.75\n",
      "Accuracy:  96.5625\n",
      "Accuracy:  94.0625\n",
      "Accuracy:  88.75\n",
      "Accuracy:  93.4375\n",
      "Accuracy:  94.6875\n",
      "Accuracy:  96.25\n",
      "Accuracy:  97.8125\n",
      "Accuracy:  92.1875\n",
      "Accuracy:  91.5625\n",
      "Accuracy:  89.0625\n",
      "Accuracy:  95.3125\n",
      "Accuracy:  93.125\n",
      "Accuracy:  94.6875\n",
      "Accuracy:  94.6875\n",
      "Accuracy:  96.5625\n",
      "Accuracy:  93.125\n",
      "Accuracy:  92.8125\n",
      "Accuracy:  96.5625\n",
      "Accuracy:  96.25\n",
      "Accuracy:  92.1875\n",
      "Accuracy:  94.6875\n",
      "Accuracy:  97.8125\n",
      "Accuracy:  89.6875\n",
      "Accuracy:  93.125\n",
      "Accuracy:  91.5625\n",
      "Accuracy:  93.125\n",
      "Accuracy:  96.5625\n",
      "Accuracy:  96.5625\n",
      "Accuracy:  95.3125\n",
      "Accuracy:  91.5625\n",
      "Accuracy:  93.125\n",
      "Accuracy:  96.5625\n",
      "Accuracy:  94.375\n",
      "Accuracy:  90.0\n",
      "Accuracy:  91.875\n",
      "Accuracy:  93.75\n",
      "Accuracy:  92.1875\n",
      "Accuracy:  93.4375\n",
      "Accuracy:  89.375\n",
      "Accuracy:  93.125\n",
      "Accuracy:  84.6875\n",
      "Accuracy:  90.625\n",
      "Accuracy:  92.8125\n",
      "Accuracy:  88.4375\n",
      "Accuracy:  90.9375\n",
      "Accuracy:  97.1875\n",
      "Accuracy:  92.1875\n",
      "Accuracy:  89.6875\n",
      "Accuracy:  96.875\n",
      "Accuracy:  88.4375\n",
      "Accuracy:  91.875\n",
      "Accuracy:  89.6875\n",
      "Accuracy:  90.625\n",
      "Accuracy:  94.6875\n",
      "Accuracy:  89.0625\n",
      "Accuracy:  95.0\n",
      "Accuracy:  93.4375\n",
      "Accuracy:  87.5\n",
      "Accuracy:  92.1875\n",
      "Accuracy:  96.5625\n",
      "Accuracy:  99.6875\n",
      "Accuracy:  93.125\n",
      "Accuracy:  93.125\n",
      "Accuracy:  94.0625\n",
      "Accuracy:  97.8125\n",
      "Accuracy:  93.4375\n",
      "Accuracy:  95.0\n",
      "Accuracy:  90.0\n",
      "Accuracy:  85.3125\n",
      "Accuracy:  94.375\n",
      "Accuracy:  88.75\n",
      "Accuracy:  97.1875\n",
      "Accuracy:  94.0625\n",
      "Accuracy:  91.5625\n",
      "Accuracy:  87.5\n",
      "Accuracy:  93.125\n",
      "Accuracy:  92.5\n",
      "Accuracy:  97.8125\n",
      "Accuracy:  87.1875\n",
      "Accuracy:  90.625\n",
      "Accuracy:  95.0\n",
      "Accuracy:  92.1875\n",
      "Accuracy:  90.9375\n",
      "Accuracy:  90.0\n",
      "Accuracy:  87.8125\n",
      "Accuracy:  91.5625\n",
      "Accuracy:  86.5625\n",
      "Accuracy:  94.375\n",
      "Accuracy:  87.5\n",
      "Accuracy:  87.8125\n",
      "Accuracy:  90.625\n",
      "Accuracy:  93.4375\n",
      "Accuracy:  91.25\n",
      "Accuracy:  89.375\n",
      "Accuracy:  93.125\n",
      "Accuracy:  85.0\n",
      "Accuracy:  89.375\n",
      "Accuracy:  98.125\n",
      "Accuracy:  93.125\n",
      "Accuracy:  96.875\n",
      "Accuracy:  93.4375\n",
      "Accuracy:  92.5\n",
      "Accuracy:  93.75\n",
      "Accuracy:  90.9375\n",
      "Accuracy:  90.9375\n",
      "Accuracy:  91.875\n",
      "Accuracy:  94.375\n",
      "Accuracy:  97.8125\n",
      "Accuracy:  95.625\n",
      "Accuracy:  87.1875\n",
      "Accuracy:  90.3125\n",
      "Accuracy:  90.0\n",
      "Accuracy:  95.625\n",
      "Accuracy:  94.6875\n",
      "Accuracy:  91.25\n",
      "Accuracy:  91.875\n",
      "Accuracy:  94.0625\n",
      "Accuracy:  94.6875\n",
      "Accuracy:  96.875\n",
      "Accuracy:  90.625\n",
      "Accuracy:  87.1875\n",
      "Accuracy:  87.5\n",
      "Accuracy:  97.1875\n",
      "Accuracy:  92.85714285714286\n"
     ]
    }
   ],
   "source": [
    "#Test\n",
    "\n",
    "# y_pred = model(list(iter(test_set)))\n",
    "\n",
    "for i,data in enumerate(testloader):\n",
    "    x,y = data\n",
    "    \n",
    "    y_pred = model(x)\n",
    "    \n",
    "    \n",
    "    y_pred = (torch.flatten(y_pred, start_dim=0, end_dim=1))\n",
    "    gt = (torch.flatten(y, start_dim=0, end_dim=1))\n",
    "    \n",
    "    \n",
    "    y_pred = y_pred.cpu()\n",
    "    y_pred = y_pred.detach().numpy()\n",
    "    \n",
    "        \n",
    "    gt = gt.cpu()\n",
    "    gt = gt.detach().numpy()\n",
    "    \n",
    "    gt_f = np.argmax(gt,axis=1)\n",
    "    y_pred_f = np.argmax(y_pred,axis=1)\n",
    "    \n",
    "    accuracy = np.mean(gt_f == y_pred_f) * 100\n",
    "    print(\"Accuracy: \", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rsurgery",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
