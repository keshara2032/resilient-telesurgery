{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-02 12:43:29.303840: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-02 12:43:29.438720: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-02 12:43:29.468513: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-08-02 12:43:30.168292: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/kesharaw/anaconda3/envs/tensorflow/lib/:/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/nvidia/cudnn/lib::/usr/local/cuda-11.7/lib64:/usr/local/cuda-11.7/lib64:/usr/local/cuda-11.7/lib64\n",
      "2023-08-02 12:43:30.168365: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/kesharaw/anaconda3/envs/tensorflow/lib/:/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/nvidia/cudnn/lib::/usr/local/cuda-11.7/lib64:/usr/local/cuda-11.7/lib64:/usr/local/cuda-11.7/lib64\n",
      "2023-08-02 12:43:30.168373: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydot in /home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages (from pydot) (3.0.9)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import glob\n",
    "\n",
    "!pip install pydot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actual Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Subject Trials:  32\n",
      "Test Subject Trials:  4\n",
      "input_dim:  (30, 66)\n",
      "output_dim:  14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-02 12:43:32.866311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-02 12:43:32.866937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-02 12:43:32.867011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-02 12:43:32.867455: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-02 12:43:32.868141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-02 12:43:32.868395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-02 12:43:32.868454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-02 12:43:34.371232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-02 12:43:34.371603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-02 12:43:34.371829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-02 12:43:34.371962: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 49 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2023-08-02 12:43:34.397638: I tensorflow/stream_executor/cuda/cuda_driver.cc:733] failed to allocate 49.06M (51445760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2023-08-02 12:43:34.398020: I tensorflow/stream_executor/cuda/cuda_driver.cc:733] failed to allocate 44.16M (46301184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2023-08-02 12:43:34.398553: I tensorflow/stream_executor/cuda/cuda_driver.cc:733] failed to allocate 39.74M (41671168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2023-08-02 12:43:34.399143: I tensorflow/stream_executor/cuda/cuda_driver.cc:733] failed to allocate 35.77M (37504256 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2023-08-02 12:43:34.399683: I tensorflow/stream_executor/cuda/cuda_driver.cc:733] failed to allocate 32.19M (33753856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2023-08-02 12:43:34.400666: I tensorflow/stream_executor/cuda/cuda_driver.cc:733] failed to allocate 28.97M (30378496 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2023-08-02 12:43:34.401253: I tensorflow/stream_executor/cuda/cuda_driver.cc:733] failed to allocate 26.07M (27340800 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2023-08-02 12:43:34.401699: I tensorflow/stream_executor/cuda/cuda_driver.cc:733] failed to allocate 23.47M (24606720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2023-08-02 12:43:34.402307: I tensorflow/stream_executor/cuda/cuda_driver.cc:733] failed to allocate 21.12M (22146048 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2023-08-02 12:43:34.402819: I tensorflow/stream_executor/cuda/cuda_driver.cc:733] failed to allocate 19.01M (19931648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2023-08-02 12:43:34.403325: I tensorflow/stream_executor/cuda/cuda_driver.cc:733] failed to allocate 17.11M (17938688 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2023-08-02 12:43:34.403835: I tensorflow/stream_executor/cuda/cuda_driver.cc:733] failed to allocate 15.40M (16144896 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2023-08-02 12:43:34.404445: I tensorflow/stream_executor/cuda/cuda_driver.cc:733] failed to allocate 13.86M (14530560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2023-08-02 12:43:34.404937: I tensorflow/stream_executor/cuda/cuda_driver.cc:733] failed to allocate 12.47M (13077504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2023-08-02 12:43:34.405306: I tensorflow/stream_executor/cuda/cuda_driver.cc:733] failed to allocate 11.22M (11769856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2023-08-02 12:43:34.405664: I tensorflow/stream_executor/cuda/cuda_driver.cc:733] failed to allocate 10.10M (10593024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2023-08-02 12:43:34.406028: I tensorflow/stream_executor/cuda/cuda_driver.cc:733] failed to allocate 9.09M (9533952 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2023-08-02 12:43:34.406393: I tensorflow/stream_executor/cuda/cuda_driver.cc:733] failed to allocate 8.18M (8580608 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2023-08-02 12:43:34.406839: I tensorflow/stream_executor/cuda/cuda_driver.cc:733] failed to allocate 7.36M (7722752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2023-08-02 12:43:34.407228: I tensorflow/stream_executor/cuda/cuda_driver.cc:733] failed to allocate 6.63M (6950656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2023-08-02 12:43:34.407706: I tensorflow/stream_executor/cuda/cuda_driver.cc:733] failed to allocate 5.97M (6255616 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2023-08-02 12:43:34.408144: I tensorflow/stream_executor/cuda/cuda_driver.cc:733] failed to allocate 5.37M (5630208 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2023-08-02 12:43:34.408599: I tensorflow/stream_executor/cuda/cuda_driver.cc:733] failed to allocate 4.83M (5067264 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2023-08-02 12:43:34.409045: I tensorflow/stream_executor/cuda/cuda_driver.cc:733] failed to allocate 4.35M (4560640 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n"
     ]
    }
   ],
   "source": [
    "## generate test / train split\n",
    "\n",
    "class CustomDataset(tf.keras.utils.Sequence):\n",
    "    def __init__(self, x_set, y_set, seq_len, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.seq_len)))\n",
    "\n",
    "        \n",
    "    def most_common(self,lst):\n",
    "        # print(\"mc\",lst)\n",
    "        lst = [np.where(r==1)[0][0] for r in lst]\n",
    "        return (np.bincount(lst).argmax())\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = idx * self.seq_len\n",
    "        end_idx = (idx + 1) * self.seq_len\n",
    "\n",
    "        batch_x = self.x[start_idx:end_idx]\n",
    "        batch_y = self.y[start_idx:end_idx]\n",
    "        \n",
    "        \n",
    "        # most_com = self.most_common(batch_y)\n",
    "        # print(most_com)\n",
    "        \n",
    "        batch_x = torch.from_numpy(batch_x)\n",
    "        batch_y = torch.from_numpy(batch_y)\n",
    "        \n",
    "        # Pad sequences to ensure they have the same length within the batch\n",
    "        pad_len = self.seq_len - batch_x.shape[0]\n",
    "        if pad_len > 0:\n",
    "            pad_shape = (pad_len,) + batch_x.shape[1:]\n",
    "            pad_shape_y = (pad_len,) + batch_y.shape[1:]\n",
    "            \n",
    "            batch_x = torch.cat([batch_x, torch.zeros(pad_shape)], dim=0)\n",
    "            batch_y = torch.cat([batch_y, torch.zeros(pad_shape_y)], dim=0)\n",
    "\n",
    "\n",
    "        return batch_x, batch_y\n",
    "\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        indices = np.arange(len(self.x))\n",
    "        np.random.shuffle(indices)\n",
    "        self.x = self.x[indices]\n",
    "        self.y = self.y[indices]\n",
    "        \n",
    "def generate_data_split(subject_id):\n",
    "    # Get CSV files list from a folder\n",
    "    train_path = './Train'\n",
    "    test_path = './Test'\n",
    "    \n",
    "    csv_path = './ProcessedDatasets/Knot_Tying'\n",
    "    \n",
    "    csv_files = glob.glob(csv_path + \"/*.csv\")\n",
    "    \n",
    "    train_df_list = []\n",
    "    test_df_list = []\n",
    "    \n",
    "    for file in csv_files:\n",
    "        if(subject_id in file):\n",
    "            test_df_list.append(pd.read_csv(file))\n",
    "#             print(file)\n",
    "        else:\n",
    "            train_df_list.append(pd.read_csv(file))\n",
    "            \n",
    "\n",
    "    print('Train Subject Trials: ',len(train_df_list))\n",
    "    print('Test Subject Trials: ',len(test_df_list))\n",
    "    \n",
    "    # Concatenate all DataFrames\n",
    "    train_df   = pd.concat(train_df_list, ignore_index=True)\n",
    "    test_df   = pd.concat(test_df_list, ignore_index=True)\n",
    "\n",
    "    \n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "\n",
    "    train_labels= train_df.pop('label')\n",
    "    train_features = train_df\n",
    "\n",
    "    test_labels= test_df.pop('label')\n",
    "    test_features = test_df\n",
    "\n",
    "\n",
    "    all_class_names = [\"G1\", 'G2', 'G3', 'G4', 'G5', 'G6', 'G8', 'G9', 'G10', 'G11', 'G12', 'G13', 'G14', 'G15']\n",
    "    lb.fit(all_class_names)\n",
    "\n",
    "    train_labels = lb.transform(train_labels)\n",
    "    test_labels = lb.transform(test_labels)\n",
    "    \n",
    "    train_x = train_features.to_numpy()\n",
    "    train_y = train_labels\n",
    "\n",
    "    test_x = test_features.to_numpy()\n",
    "    test_y = test_labels\n",
    "\n",
    "    seq_len = 30\n",
    "    batch_size = 64\n",
    "\n",
    "    valid_test_split = 0.8\n",
    "    # Step 2: Split the remaining data into validation and test sets\n",
    "    val_x, test_x, val_y, test_y = train_test_split(\n",
    "    test_x, test_y, test_size=valid_test_split, random_state=42)\n",
    "    \n",
    "    train_dataset = CustomDataset(train_x, train_y, seq_len, batch_size)\n",
    "    \n",
    "    val_dataset = CustomDataset(val_x, val_y, seq_len, batch_size)\n",
    "    \n",
    "    test_dataset = CustomDataset(test_x, test_y, seq_len, batch_size)\n",
    "\n",
    "    train_dataloader = tf.data.Dataset.from_generator(\n",
    "        lambda: train_dataset,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=( seq_len, train_x.shape[1]), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=( seq_len, train_y.shape[1]), dtype=tf.float32),\n",
    "        ),\n",
    "    # )\n",
    "    ).repeat()\n",
    "    train_dataloader = train_dataloader.batch(batch_size)\n",
    "\n",
    "\n",
    "    val_dataloader = tf.data.Dataset.from_generator(\n",
    "        lambda: val_dataset,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=( seq_len, test_x.shape[1]), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=( seq_len, test_y.shape[1]), dtype=tf.float32),\n",
    "        ),\n",
    "    )\n",
    "    val_dataloader = val_dataloader.batch(batch_size)\n",
    "    \n",
    "\n",
    "    test_dataloader = tf.data.Dataset.from_generator(\n",
    "        lambda: test_dataset,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=( seq_len, test_x.shape[1]), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=( seq_len, test_y.shape[1]), dtype=tf.float32),\n",
    "        ),\n",
    "    )\n",
    "    test_dataloader = test_dataloader.batch(batch_size)\n",
    "    \n",
    "    return train_dataloader, val_dataloader, test_dataloader\n",
    "    \n",
    "subjects = ['S02','S03','S04','S05','S06','S07','S08','S09']\n",
    "\n",
    "for subject in subjects:\n",
    "    train_dataloader, val_dataloader, test_dataloader = generate_data_split(subject)\n",
    "    train_features, train_labels = next(iter(val_dataloader))\n",
    "\n",
    "    input_shape = train_features.shape[1:]\n",
    "    output_dim = train_labels.shape[2]\n",
    "\n",
    "    print(\"input_dim: \",input_shape)\n",
    "    print(\"output_dim: \",output_dim)\n",
    "    \n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Subject Trials:  31\n",
      "Test Subject Trials:  5\n",
      "input_dim:  (30, 66)\n",
      "output_dim:  14\n"
     ]
    }
   ],
   "source": [
    "def make_model(input_shape, num_classes):\n",
    "    input_layer = keras.layers.Input(input_shape)\n",
    "    \n",
    "    conv1 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(input_layer)\n",
    "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = keras.layers.ReLU()(conv1)\n",
    "\n",
    "    conv2 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv1)\n",
    "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = keras.layers.ReLU()(conv2)\n",
    "\n",
    "    conv3 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv2)\n",
    "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = keras.layers.ReLU()(conv3)\n",
    "\n",
    "    gap = keras.layers.GlobalAveragePooling1D(data_format=\"channels_first\", keepdims=True)(conv3)\n",
    "\n",
    "    output_layer = keras.layers.Dense(num_classes, activation=\"softmax\")(gap)\n",
    "\n",
    "    return keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader = generate_data_split('S04')\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "\n",
    "input_shape = train_features.shape[1:]\n",
    "output_dim = train_labels.shape[2]\n",
    "\n",
    "print(\"input_dim: \",input_shape)\n",
    "print(\"output_dim: \",output_dim)\n",
    "\n",
    "\n",
    "model = make_model(input_shape=input_shape, num_classes=output_dim)\n",
    "# keras.utils.plot_model(model, show_shapes=True)\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " *** --------START TRAIN--------- *** \n",
      "SUBJECT EXCLUDED:  S02\n",
      "Train Subject Trials:  32\n",
      "Test Subject Trials:  4\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-02 12:43:46.139943: I tensorflow/stream_executor/cuda/cuda_driver.cc:733] failed to allocate 45.15M (47341056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2023-08-02 12:43:46.139997: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 16.47MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-08-02 12:43:46.140010: W tensorflow/core/kernels/gpu_utils.cc:50] Failed to allocate memory for convolution redzone checking; skipping this check. This is benign and only means that we won't check cudnn for out-of-bounds reads and writes. This message will only be printed once.\n",
      "2023-08-02 12:43:46.201094: E tensorflow/stream_executor/cuda/cuda_dnn.cc:389] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\n",
      "2023-08-02 12:43:46.201156: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at conv_ops.cc:1134 : UNIMPLEMENTED: DNN library is not found.\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "Graph execution error:\n\nDetected at node 'model/conv1d/Conv1D' defined at (most recent call last):\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 728, in start\n      self.io_loop.start()\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n      await self.process_one()\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n      await dispatch(*args)\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n      await result\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n      reply_content = await reply_content\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n      result = self._run_cell(\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n      result = runner(coro)\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_779377/3041385939.py\", line 37, in <module>\n      history = model.fit(\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/layers/convolutional/base_conv.py\", line 283, in call\n      outputs = self.convolution_op(inputs, self.kernel)\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/layers/convolutional/base_conv.py\", line 255, in convolution_op\n      return tf.nn.convolution(\nNode: 'model/conv1d/Conv1D'\nDNN library is not found.\n\t [[{{node model/conv1d/Conv1D}}]] [Op:__inference_train_function_2181]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 37\u001b[0m\n\u001b[1;32m     33\u001b[0m train_dataloader, val_dataloader, test_dataloader \u001b[39m=\u001b[39m generate_data_split(subject)\n\u001b[1;32m     35\u001b[0m model\u001b[39m.\u001b[39mload_weights(\u001b[39m'\u001b[39m\u001b[39m./checkpoints/blank_state\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     38\u001b[0m     train_dataloader,\n\u001b[1;32m     39\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m     40\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m     41\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[1;32m     42\u001b[0m     validation_data\u001b[39m=\u001b[39;49mval_dataloader,\n\u001b[1;32m     43\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     46\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m *** -------END TRAIN-------- *** \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m *** -------START EVAL-------- *** \u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnimplementedError\u001b[0m: Graph execution error:\n\nDetected at node 'model/conv1d/Conv1D' defined at (most recent call last):\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 728, in start\n      self.io_loop.start()\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n      await self.process_one()\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n      await dispatch(*args)\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n      await result\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n      reply_content = await reply_content\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n      result = self._run_cell(\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n      result = runner(coro)\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_779377/3041385939.py\", line 37, in <module>\n      history = model.fit(\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/layers/convolutional/base_conv.py\", line 283, in call\n      outputs = self.convolution_op(inputs, self.kernel)\n    File \"/home/kesharaw/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/layers/convolutional/base_conv.py\", line 255, in convolution_op\n      return tf.nn.convolution(\nNode: 'model/conv1d/Conv1D'\nDNN library is not found.\n\t [[{{node model/conv1d/Conv1D}}]] [Op:__inference_train_function_2181]"
     ]
    }
   ],
   "source": [
    "epochs = 25\n",
    "steps_per_epoch = 500\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        \"best_model.h5\", save_best_only=True, monitor=\"val_loss\"\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=20, min_lr=0.0001\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, verbose=1),\n",
    "]\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"categorical_accuracy\"],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "subjects = ['S02','S03','S04','S05','S06','S07','S08','S09']\n",
    "\n",
    "\n",
    "model.save_weights('./checkpoints/blank_state')\n",
    "\n",
    "for subject in subjects:\n",
    "    \n",
    "    print(\" *** --------START TRAIN--------- *** \")\n",
    "    print(\"SUBJECT EXCLUDED: \",subject)\n",
    "    \n",
    "    train_dataloader, val_dataloader, test_dataloader = generate_data_split(subject)\n",
    "\n",
    "    model.load_weights('./checkpoints/blank_state')\n",
    "\n",
    "    history = model.fit(\n",
    "        train_dataloader,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=val_dataloader,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    print(\" *** -------END TRAIN-------- *** \")\n",
    "    print(\" *** -------START EVAL-------- *** \")\n",
    "    \n",
    "    model.evaluate(test_dataloader, verbose=1)\n",
    "\n",
    "    print(\" *** -------END EVAL-------- *** \")\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1/Unknown - 0s 20ms/step - loss: 7.4180 - categorical_accuracy: 0.2594"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 9ms/step - loss: 7.4923 - categorical_accuracy: 0.2656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7.492281436920166, 0.2655773460865021]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_dataloader, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Subject Trials:  32\n",
      "Test Subject Trials:  4\n",
      "input_dim:  (30, 66)\n",
      "output_dim:  14\n",
      "14\n",
      " *** --------START--------- *** \n",
      "SUBJECT EXCLUDED:  S02\n",
      "Train Subject Trials:  32\n",
      "Test Subject Trials:  4\n",
      "Epoch 1/50\n",
      "499/500 [============================>.] - ETA: 0s - loss: 8.4796 - categorical_accuracy: 0.2081WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 11s 17ms/step - loss: 8.4678 - categorical_accuracy: 0.2082\n",
      "Epoch 2/50\n",
      "498/500 [============================>.] - ETA: 0s - loss: 2.1731 - categorical_accuracy: 0.2531WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 2.1732 - categorical_accuracy: 0.2531\n",
      "Epoch 3/50\n",
      "499/500 [============================>.] - ETA: 0s - loss: 1.9201 - categorical_accuracy: 0.2686WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 1.9199 - categorical_accuracy: 0.2687\n",
      "Epoch 4/50\n",
      "499/500 [============================>.] - ETA: 0s - loss: 1.7386 - categorical_accuracy: 0.2890WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 1.7385 - categorical_accuracy: 0.2889\n",
      "Epoch 5/50\n",
      "498/500 [============================>.] - ETA: 0s - loss: 1.6056 - categorical_accuracy: 0.3183WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 1.6053 - categorical_accuracy: 0.3186\n",
      "Epoch 6/50\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.4901 - categorical_accuracy: 0.3880WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 1.4901 - categorical_accuracy: 0.3880\n",
      "Epoch 7/50\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.4061 - categorical_accuracy: 0.4259WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 1.4061 - categorical_accuracy: 0.4259\n",
      "Epoch 8/50\n",
      "499/500 [============================>.] - ETA: 0s - loss: 1.3554 - categorical_accuracy: 0.4448WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 1.3554 - categorical_accuracy: 0.4449\n",
      "Epoch 9/50\n",
      "497/500 [============================>.] - ETA: 0s - loss: 1.3163 - categorical_accuracy: 0.4602WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 1.3169 - categorical_accuracy: 0.4599\n",
      "Epoch 10/50\n",
      "498/500 [============================>.] - ETA: 0s - loss: 1.2808 - categorical_accuracy: 0.4742WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 1.2815 - categorical_accuracy: 0.4739\n",
      "Epoch 11/50\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.2494 - categorical_accuracy: 0.4866WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 1.2494 - categorical_accuracy: 0.4866\n",
      "Epoch 12/50\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.2191 - categorical_accuracy: 0.4972WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 1.2191 - categorical_accuracy: 0.4972\n",
      "Epoch 13/50\n",
      "498/500 [============================>.] - ETA: 0s - loss: 1.1865 - categorical_accuracy: 0.5063WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 1.1866 - categorical_accuracy: 0.5063\n",
      "Epoch 14/50\n",
      "498/500 [============================>.] - ETA: 0s - loss: 1.1556 - categorical_accuracy: 0.5132WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 1.1552 - categorical_accuracy: 0.5132\n",
      "Epoch 15/50\n",
      "499/500 [============================>.] - ETA: 0s - loss: 1.1250 - categorical_accuracy: 0.5194WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 1.1252 - categorical_accuracy: 0.5193\n",
      "Epoch 16/50\n",
      "497/500 [============================>.] - ETA: 0s - loss: 1.0968 - categorical_accuracy: 0.5232WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 1.0966 - categorical_accuracy: 0.5235\n",
      "Epoch 17/50\n",
      "498/500 [============================>.] - ETA: 0s - loss: 1.0697 - categorical_accuracy: 0.5292WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 1.0702 - categorical_accuracy: 0.5289\n",
      "Epoch 18/50\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.0446 - categorical_accuracy: 0.5366WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 1.0446 - categorical_accuracy: 0.5366\n",
      "Epoch 19/50\n",
      "498/500 [============================>.] - ETA: 0s - loss: 1.0244 - categorical_accuracy: 0.5458WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 1.0240 - categorical_accuracy: 0.5461\n",
      "Epoch 20/50\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.0040 - categorical_accuracy: 0.5569WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 1.0040 - categorical_accuracy: 0.5569\n",
      "Epoch 21/50\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.9828 - categorical_accuracy: 0.5704WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.9825 - categorical_accuracy: 0.5706\n",
      "Epoch 22/50\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.9625 - categorical_accuracy: 0.5862WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.9625 - categorical_accuracy: 0.5862\n",
      "Epoch 23/50\n",
      "497/500 [============================>.] - ETA: 0s - loss: 0.9400 - categorical_accuracy: 0.6030WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.9391 - categorical_accuracy: 0.6037\n",
      "Epoch 24/50\n",
      "500/500 [==============================] - ETA: 0s - loss: 0.9181 - categorical_accuracy: 0.6174WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.9181 - categorical_accuracy: 0.6174\n",
      "Epoch 25/50\n",
      "500/500 [==============================] - ETA: 0s - loss: 0.8934 - categorical_accuracy: 0.6330WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.8934 - categorical_accuracy: 0.6330\n",
      "Epoch 26/50\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.8681 - categorical_accuracy: 0.6478WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.8679 - categorical_accuracy: 0.6479\n",
      "Epoch 27/50\n",
      "498/500 [============================>.] - ETA: 0s - loss: 0.8431 - categorical_accuracy: 0.6597WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.8431 - categorical_accuracy: 0.6597\n",
      "Epoch 28/50\n",
      "498/500 [============================>.] - ETA: 0s - loss: 0.8236 - categorical_accuracy: 0.6715WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.8229 - categorical_accuracy: 0.6718\n",
      "Epoch 29/50\n",
      "500/500 [==============================] - ETA: 0s - loss: 0.7964 - categorical_accuracy: 0.6849WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.7964 - categorical_accuracy: 0.6849\n",
      "Epoch 30/50\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.7709 - categorical_accuracy: 0.6978WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.7716 - categorical_accuracy: 0.6975\n",
      "Epoch 31/50\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.7450 - categorical_accuracy: 0.7103WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.7448 - categorical_accuracy: 0.7104\n",
      "Epoch 32/50\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.7224 - categorical_accuracy: 0.7219WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.7223 - categorical_accuracy: 0.7219\n",
      "Epoch 33/50\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.6962 - categorical_accuracy: 0.7331WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.6965 - categorical_accuracy: 0.7329\n",
      "Epoch 34/50\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.6696 - categorical_accuracy: 0.7452WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.6696 - categorical_accuracy: 0.7452\n",
      "Epoch 35/50\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.6465 - categorical_accuracy: 0.7563WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.6466 - categorical_accuracy: 0.7562\n",
      "Epoch 36/50\n",
      "500/500 [==============================] - ETA: 0s - loss: 0.6249 - categorical_accuracy: 0.7662WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.6249 - categorical_accuracy: 0.7662\n",
      "Epoch 37/50\n",
      "498/500 [============================>.] - ETA: 0s - loss: 0.6053 - categorical_accuracy: 0.7760WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.6061 - categorical_accuracy: 0.7756\n",
      "Epoch 38/50\n",
      "497/500 [============================>.] - ETA: 0s - loss: 0.5867 - categorical_accuracy: 0.7848WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.5863 - categorical_accuracy: 0.7850\n",
      "Epoch 39/50\n",
      "500/500 [==============================] - ETA: 0s - loss: 0.5720 - categorical_accuracy: 0.7917WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.5720 - categorical_accuracy: 0.7917\n",
      "Epoch 40/50\n",
      "500/500 [==============================] - ETA: 0s - loss: 0.5582 - categorical_accuracy: 0.7991WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.5582 - categorical_accuracy: 0.7991\n",
      "Epoch 41/50\n",
      "500/500 [==============================] - ETA: 0s - loss: 0.5408 - categorical_accuracy: 0.8061WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.5408 - categorical_accuracy: 0.8061\n",
      "Epoch 42/50\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.5564 - categorical_accuracy: 0.7992WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.5577 - categorical_accuracy: 0.7987\n",
      "Epoch 43/50\n",
      "498/500 [============================>.] - ETA: 0s - loss: 0.5497 - categorical_accuracy: 0.8038WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.5492 - categorical_accuracy: 0.8040\n",
      "Epoch 44/50\n",
      "498/500 [============================>.] - ETA: 0s - loss: 0.4894 - categorical_accuracy: 0.8297WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.4890 - categorical_accuracy: 0.8298\n",
      "Epoch 45/50\n",
      "497/500 [============================>.] - ETA: 0s - loss: 0.4664 - categorical_accuracy: 0.8392WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.4669 - categorical_accuracy: 0.8390\n",
      "Epoch 46/50\n",
      "500/500 [==============================] - ETA: 0s - loss: 0.4465 - categorical_accuracy: 0.8472WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.4465 - categorical_accuracy: 0.8472\n",
      "Epoch 47/50\n",
      "498/500 [============================>.] - ETA: 0s - loss: 0.4278 - categorical_accuracy: 0.8552WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.4282 - categorical_accuracy: 0.8550\n",
      "Epoch 48/50\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.4171 - categorical_accuracy: 0.8593WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.4170 - categorical_accuracy: 0.8595\n",
      "Epoch 49/50\n",
      "498/500 [============================>.] - ETA: 0s - loss: 0.4017 - categorical_accuracy: 0.8663WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.4015 - categorical_accuracy: 0.8664\n",
      "Epoch 50/50\n",
      "500/500 [==============================] - ETA: 0s - loss: 0.3836 - categorical_accuracy: 0.8737WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.3836 - categorical_accuracy: 0.8737\n",
      " *** -------END-------- *** \n",
      " *** --------START--------- *** \n",
      "SUBJECT EXCLUDED:  S03\n",
      "Train Subject Trials:  31\n",
      "Test Subject Trials:  5\n",
      "Epoch 1/50\n",
      "498/500 [============================>.] - ETA: 0s - loss: 0.5966 - categorical_accuracy: 0.8091WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.5963 - categorical_accuracy: 0.8092\n",
      "Epoch 2/50\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.5172 - categorical_accuracy: 0.8268WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.5172 - categorical_accuracy: 0.8268\n",
      "Epoch 3/50\n",
      "498/500 [============================>.] - ETA: 0s - loss: 0.4869 - categorical_accuracy: 0.8363WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.4870 - categorical_accuracy: 0.8363\n",
      "Epoch 4/50\n",
      "500/500 [==============================] - ETA: 0s - loss: 0.4806 - categorical_accuracy: 0.8369WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.4806 - categorical_accuracy: 0.8369\n",
      "Epoch 5/50\n",
      "498/500 [============================>.] - ETA: 0s - loss: 0.5238 - categorical_accuracy: 0.8174WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.5241 - categorical_accuracy: 0.8173\n",
      "Epoch 6/50\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.4749 - categorical_accuracy: 0.8390WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.4751 - categorical_accuracy: 0.8389\n",
      "Epoch 7/50\n",
      "498/500 [============================>.] - ETA: 0s - loss: 0.4119 - categorical_accuracy: 0.8629WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.4121 - categorical_accuracy: 0.8628\n",
      "Epoch 8/50\n",
      "500/500 [==============================] - ETA: 0s - loss: 0.4079 - categorical_accuracy: 0.8641WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.4079 - categorical_accuracy: 0.8641\n",
      "Epoch 9/50\n",
      "476/500 [===========================>..] - ETA: 0s - loss: 0.3941 - categorical_accuracy: 0.8682"
     ]
    }
   ],
   "source": [
    "# input_shape = (30,66)\n",
    "# input_shape = train_features.shape\n",
    "\n",
    "# train_features = train_features.to_numpy()\n",
    "# train_features = train_features.reshape((train_features.shape[0], train_features.shape[1], 1))\n",
    "# input_shape = train_features.shape\n",
    "train_dataloader, test_dataloader = generate_data_split(subject)\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "\n",
    "input_shape = train_features.shape[1:]\n",
    "output_dim = train_labels.shape[2]\n",
    "\n",
    "print(\"input_dim: \",input_shape)\n",
    "print(\"output_dim: \",output_dim)\n",
    "\n",
    "\n",
    "\n",
    "model = build_model(\n",
    "    input_shape,\n",
    "    output_dim,\n",
    "    head_size=256,\n",
    "    num_heads=2,\n",
    "    ff_dim=512,\n",
    "    num_transformer_blocks=4,\n",
    "    mlp_units=[256,128],\n",
    "    mlp_dropout=0.5,\n",
    "    dropout=0.3,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    metrics=[\"categorical_accuracy\"],\n",
    ")\n",
    "# # # # model.summary()\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
    "\n",
    "epochs = 50\n",
    "# steps_per_epoch = train_dataloader.__len__()//1\n",
    "# print(\"steps_per\", steps_per_epoch)\n",
    "\n",
    "\n",
    "subjects = ['S02','S03','S04','S05','S06','S07','S08','S09']\n",
    "\n",
    "for subject in subjects:\n",
    "    \n",
    "    print(\" *** --------START--------- *** \")\n",
    "    print(\"SUBJECT EXCLUDED: \",subject)\n",
    "    \n",
    "    train_dataloader, test_dataloader = generate_data_split(subject)\n",
    "\n",
    "    model.fit(\n",
    "        train_dataloader,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        steps_per_epoch=500\n",
    "#         validation_data = test_dataloader\n",
    "    )\n",
    "\n",
    "\n",
    "    print(\" *** -------END-------- *** \")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "## Pytorch dataloader implementation\n",
    "\n",
    "# def gen(torch_loader):\n",
    "#     for x,y in torch_loader:\n",
    "#         yield (x,y)\n",
    "\n",
    "# train = gen(train_dataloader)\n",
    "\n",
    "# epochs = 300\n",
    "# steps_per_epoch = train_dataloader.__len__()//5\n",
    "# print(\"steps_per\", steps_per_epoch)\n",
    "\n",
    "# model.fit(\n",
    "#     train,\n",
    "#     epochs=epochs,\n",
    "#     callbacks=callbacks,\n",
    "#     steps_per_epoch=steps_per_epoch\n",
    "# )\n",
    "\n",
    "# # model.evaluate(x_test, y_test, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install -q -U keras-tuner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dim:  (30, 71)\n",
      "output_dim:  14\n",
      "INFO:tensorflow:Reloading Tuner from ./intro_to_kt/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "# import keras_tuner as kt\n",
    "\n",
    "# train_features, train_labels = next(iter(train_dataloader))\n",
    "\n",
    "# input_shape = train_features.shape[1:]\n",
    "# output_dim = train_labels.shape[2]\n",
    "\n",
    "# print(\"input_dim: \",input_shape)\n",
    "# print(\"output_dim: \",output_dim)\n",
    "\n",
    "\n",
    "# def model_builder(hp):\n",
    "    \n",
    "    \n",
    "#     hp_headsize = hp.Int('units', min_value=16, max_value=512, step=16)\n",
    "#     hp_numheads = hp.Int('units', min_value=2, max_value=16, step=2)\n",
    "#     hp_ff_dim = hp.Int('units', min_value=1, max_value=512, step=16)\n",
    "#     hp_num_transformer_blocks = hp.Int('units', min_value=1, max_value=16, step=2)\n",
    "\n",
    "#     model = build_model(\n",
    "#     input_shape,\n",
    "#     output_dim,\n",
    "#     head_size=hp_headsize,\n",
    "#     num_heads=hp_numheads,\n",
    "#     ff_dim=hp_ff_dim,\n",
    "#     num_transformer_blocks=hp_num_transformer_blocks,\n",
    "#     mlp_units=[128],\n",
    "#     mlp_dropout=0.4,\n",
    "#     dropout=0.25,\n",
    "#     )\n",
    "\n",
    "#     model.compile(\n",
    "#         loss=\"categorical_crossentropy\",\n",
    "#         optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "#         metrics=[\"categorical_accuracy\"],\n",
    "#     )\n",
    "    \n",
    "#     return model\n",
    "\n",
    "\n",
    "# tuner = kt.Hyperband(model_builder,\n",
    "#                      objective='val_accuracy',\n",
    "#                      max_epochs=10,\n",
    "#                      factor=3,\n",
    "#                      project_name='intro_to_kt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuner.search(train_dataloader, epochs=10, steps_per_epoch=500, callbacks=[stop_early])\n",
    "\n",
    "# # Get the optimal hyperparameters\n",
    "# best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# print(f\"\"\"\n",
    "# The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "# layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
    "# is {best_hps.get('learning_rate')}.\n",
    "# \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 14ms/step - loss: 0.9062 - categorical_accuracy: 0.6734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9061856865882874, 0.6734204888343811]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_dataloader, verbose=1)\n",
    "\n",
    "# test_features, test_labels = next(iter(test_dataloader))\n",
    "\n",
    "# y_pred = model(test_features)\n",
    "\n",
    "# relu = nn.ReLU()\n",
    "\n",
    "\n",
    "# for idx,y in enumerate(y_pred) :\n",
    "#     for i,sample in enumerate(y):\n",
    "#         print(np.argmax(test_labels[idx][i].numpy()))\n",
    "#         print(np.argmax(sample))\n",
    "#         print('-----')\n",
    "        \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./checkpoints/progress_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Received incompatible tensor with shape (71,) when attempting to restore variable with shape (66,) and name layer_with_weights-0/beta/.ATTRIBUTES/VARIABLE_VALUE.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [117], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Restore the weights\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39mload_weights(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./checkpoints/progress_v1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/training/saving/saveable_object_util.py:135\u001b[0m, in \u001b[0;36mResourceVariableSaveable.restore\u001b[0;34m(self, restored_tensors, restored_shapes)\u001b[0m\n\u001b[1;32m    132\u001b[0m   assigned_variable \u001b[38;5;241m=\u001b[39m resource_variable_ops\u001b[38;5;241m.\u001b[39mshape_safe_assign_variable_handle(\n\u001b[1;32m    133\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_op, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_shape, restored_tensor)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 135\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    136\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived incompatible tensor with shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrestored_tensor\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhen attempting to restore variable with shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    138\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m assigned_variable\n",
      "\u001b[0;31mValueError\u001b[0m: Received incompatible tensor with shape (71,) when attempting to restore variable with shape (66,) and name layer_with_weights-0/beta/.ATTRIBUTES/VARIABLE_VALUE."
     ]
    }
   ],
   "source": [
    "# Restore the weights\n",
    "model.load_weights('./checkpoints/progress_v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
